{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4538d736",
   "metadata": {},
   "source": [
    "# Tradutor PortuguÃªs/InglÃªs utilizando Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e762596",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "97f26487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Mixed Precision ativado: mixed_float16\n",
      "   Compute dtype: float16\n",
      "   Variable dtype: float32\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import string\n",
    "import re\n",
    "\n",
    "os.environ['KERAS_BACKEND'] = 'tensorflow'\n",
    "\n",
    "import keras\n",
    "from keras import layers\n",
    "import tensorflow as tf\n",
    "\n",
    "# ===== OTIMIZAÃ‡ÃƒO: Mixed Precision para acelerar treinamento =====\n",
    "# Reduz uso de memÃ³ria e acelera cÃ¡lculos em GPUs modernas\n",
    "from keras import mixed_precision\n",
    "policy = mixed_precision.Policy('mixed_float16')\n",
    "mixed_precision.set_global_policy(policy)\n",
    "print(f\"âœ… Mixed Precision ativado: {policy.name}\")\n",
    "print(f\"   Compute dtype: {policy.compute_dtype}\")\n",
    "print(f\"   Variable dtype: {policy.variable_dtype}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53b23c8",
   "metadata": {},
   "source": [
    "## Carregamento dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0a511920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de pares de frases carregados: 168903\n",
      "------------------------------------------------------------\n",
      "Primeiras 3 frases em inglÃªs: ['Go.', 'Go.', 'Hi.']\n",
      "Primeiras 3 frases em portuguÃªs: ['Vai.', 'VÃ¡.', 'Oi.']\n",
      "------------------------------------------------------------\n",
      "Ãšltimas 3 frases em inglÃªs: [\"No matter how much you try to convince people that chocolate is vanilla, it'll still be chocolate, even though you may manage to convince yourself and a few others that it's vanilla.\", 'Some movies make such an impact that one never forgets them. Such is the case with \"Life is Beautiful,\" the emotional Benigni film that mixes drama and comedy in an exceptional manner.', 'A child who is a native speaker usually knows many things about his or her language that a non-native speaker who has been studying for years still does not know and perhaps will never know.']\n",
      "Ãšltimas 3 frases em portuguÃªs: ['NÃ£o importa o quanto vocÃª tenta convencer os outros de que chocolate Ã© baunilha, ele ainda serÃ¡ chocolate, mesmo que vocÃª possa convencer a si mesmo e poucos outros de que Ã© baunilha.', 'Alguns filmes sÃ£o tÃ£o marcantes que jamais nos saem da lembranÃ§a. Ã‰ o caso da emocionante pelÃ­cula de Benigni, \"La vita Ã¨ bella\", que mistura drama e comÃ©dia de maneira excepcional.', 'Uma crianÃ§a que Ã© falante nativa geralmente sabe muitas coisas sobre sua lÃ­ngua que um falante nÃ£o-nativo que tem estudado hÃ¡ anos ainda nÃ£o sabe e talvez nunca saberÃ¡.']\n"
     ]
    }
   ],
   "source": [
    "# Vamos usar listas normais do Python, que sÃ£o perfeitas para isso.\n",
    "portuguese_sentences = []\n",
    "english_sentences = []\n",
    "\n",
    "# O 'with open(...)' garante que o arquivo seja fechado corretamente no final.\n",
    "with open('por.txt', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        # Ignora linhas em branco que possam existir no arquivo\n",
    "        if not line.strip():\n",
    "            continue\n",
    "\n",
    "        # 1. strip() remove espaÃ§os/quebras de linha no inÃ­cio/fim\n",
    "        # 2. split('\\t') quebra a linha no caractere de tabulaÃ§Ã£o\n",
    "        # O novo formato tem 3 colunas: inglÃªs, portuguÃªs, metadados\n",
    "        parts = line.strip().split('\\t')\n",
    "        \n",
    "        # Verificar se a linha tem pelo menos 2 colunas (inglÃªs e portuguÃªs)\n",
    "        if len(parts) < 2:\n",
    "            continue\n",
    "        \n",
    "        english_part = parts[0]  # Primeira coluna: inglÃªs\n",
    "        portuguese_part = parts[1]  # Segunda coluna: portuguÃªs\n",
    "        # parts[2] contÃ©m os metadados, que vamos ignorar\n",
    "\n",
    "        english_sentences.append(english_part)\n",
    "        portuguese_sentences.append(portuguese_part)\n",
    "\n",
    "# Vamos conferir quantas frases temos e as 3 primeiras de cada lista\n",
    "print(f\"Total de pares de frases carregados: {len(english_sentences)}\")\n",
    "print(\"-\" * 60)\n",
    "print(\"Primeiras 3 frases em inglÃªs:\", english_sentences[:3])\n",
    "print(\"Primeiras 3 frases em portuguÃªs:\", portuguese_sentences[:3])\n",
    "print(\"-\" * 60)\n",
    "print(\"Ãšltimas 3 frases em inglÃªs:\", english_sentences[-3:])\n",
    "print(\"Ãšltimas 3 frases em portuguÃªs:\", portuguese_sentences[-3:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098d116d",
   "metadata": {},
   "source": [
    "## PrÃ©-Processamento: Limpeza, PadronizaÃ§Ã£o e TransformaÃ§Ã£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c57c976b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original English phrase: 'Hi.'\n",
      "Final target phrase:   '[start] hi [end]'\n",
      "--------------------\n",
      "Final source phrases (first 3): ['vai', 'vÃ¡', 'oi']\n",
      "Final target phrases (first 3): ['[start] go [end]', '[start] go [end]', '[start] hi [end]']\n"
     ]
    }
   ],
   "source": [
    "# Let's keep our original lists for comparison\n",
    "# portuguese_sentences = ['VÃ¡.', 'Oi.', 'Corra!', ...]\n",
    "# english_sentences = ['Go.', 'Hi.', 'Run!', ...]\n",
    "\n",
    "def standardize_text(text):\n",
    "    \"\"\"\n",
    "    Cleans and standardizes a single sentence.\n",
    "    1. Converts to lowercase.\n",
    "    2. Removes punctuation.\n",
    "    3. Removes extra whitespace.\n",
    "    \"\"\"\n",
    "    # Lowercase the text\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Create a translation table to remove punctuation\n",
    "    # string.punctuation contains characters like '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    text = text.translate(translator)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# 1. Standardize the source (Portuguese) and target (English) phrases\n",
    "source_phrases_clean = [standardize_text(pt) for pt in portuguese_sentences]\n",
    "target_phrases_clean = [standardize_text(en) for en in english_sentences]\n",
    "\n",
    "# 2. NOW, add the special tokens to the CLEANED target phrases\n",
    "target_phrases_final = [f'[start] {eng} [end]' for eng in target_phrases_clean]\n",
    "\n",
    "# Let's see the transformation for one example\n",
    "print(f\"Original English phrase: '{english_sentences[2]}'\")\n",
    "print(f\"Final target phrase:   '{target_phrases_final[2]}'\")\n",
    "print(\"-\" * 20)\n",
    "print(\"Final source phrases (first 3):\", source_phrases_clean[:3])\n",
    "print(\"Final target phrases (first 3):\", target_phrases_final[:3])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314279ac",
   "metadata": {},
   "source": [
    "## VetorizaÃ§Ã£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5923efd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ðŸ“Š ANÃLISE DO DATASET:\n",
      "============================================================\n",
      "PortuguÃªs - Comprimento mÃ©dio: 6.0 palavras\n",
      "PortuguÃªs - Comprimento mÃ¡ximo: 33 palavras\n",
      "PortuguÃªs - Percentil 95: 10 palavras\n",
      "------------------------------------------------------------\n",
      "InglÃªs - Comprimento mÃ©dio: 8.0 palavras\n",
      "InglÃªs - Comprimento mÃ¡ximo: 37 palavras\n",
      "InglÃªs - Percentil 95: 12 palavras\n",
      "============================================================\n",
      "\n",
      "ðŸŽ¯ CONFIGURAÃ‡Ã•ES CALCULADAS:\n",
      "============================================================\n",
      "âœ… Sequence Length: 14\n",
      "âœ… VocabulÃ¡rio PortuguÃªs: 22460 tokens\n",
      "âœ… VocabulÃ¡rio InglÃªs: 12908 tokens\n",
      "============================================================\n",
      "\n",
      "ðŸ“ TESTE DE VETORIZAÃ‡ÃƒO:\n",
      "Original Portuguese: 'que'\n",
      "Vectorized: [[3 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
      "------------------------------------------------------------\n",
      "Original English: '[start] who [end]'\n",
      "Vectorized: [[ 2 68  3  0  0  0  0  0  0  0  0  0  0  0]]\n",
      "\n",
      "ðŸ” Primeiras 10 palavras do vocabulÃ¡rio:\n",
      "PortuguÃªs: ['', '[UNK]', np.str_('tom'), np.str_('que'), np.str_('o'), np.str_('nÃ£o'), np.str_('eu'), np.str_('de'), np.str_('a'), np.str_('vocÃª')]\n",
      "InglÃªs: ['', '[UNK]', np.str_('[start]'), np.str_('[end]'), np.str_('tom'), np.str_('i'), np.str_('to'), np.str_('you'), np.str_('the'), np.str_('a')]\n",
      "\n",
      "âœ… '[start]' no vocabulÃ¡rio inglÃªs: True\n",
      "âœ… '[end]' no vocabulÃ¡rio inglÃªs: True\n"
     ]
    }
   ],
   "source": [
    "# OTIMIZAÃ‡ÃƒO: Calcular vocab_size e sequence_length dinamicamente do dataset\n",
    "# Isso garante que o modelo se adapte aos dados reais\n",
    "\n",
    "# Primeiro, vamos analisar os dados para determinar o tamanho mÃ¡ximo de sequÃªncia\n",
    "import numpy as np\n",
    "\n",
    "# Calcular o comprimento de cada frase (em palavras)\n",
    "source_lengths = [len(phrase.split()) for phrase in source_phrases_clean]\n",
    "target_lengths = [len(phrase.split()) for phrase in target_phrases_final]\n",
    "\n",
    "# EstatÃ­sticas dos comprimentos\n",
    "print(\"=\" * 60)\n",
    "print(\"ðŸ“Š ANÃLISE DO DATASET:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"PortuguÃªs - Comprimento mÃ©dio: {np.mean(source_lengths):.1f} palavras\")\n",
    "print(f\"PortuguÃªs - Comprimento mÃ¡ximo: {np.max(source_lengths)} palavras\")\n",
    "print(f\"PortuguÃªs - Percentil 95: {np.percentile(source_lengths, 95):.0f} palavras\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"InglÃªs - Comprimento mÃ©dio: {np.mean(target_lengths):.1f} palavras\")\n",
    "print(f\"InglÃªs - Comprimento mÃ¡ximo: {np.max(target_lengths)} palavras\")\n",
    "print(f\"InglÃªs - Percentil 95: {np.percentile(target_lengths, 95):.0f} palavras\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Definir sequence_length como o percentil 95 (cobre 95% dos dados)\n",
    "# Adicionar margem de seguranÃ§a (+2 para os tokens [start] e [end])\n",
    "sequence_length = int(max(\n",
    "    np.percentile(source_lengths, 95),\n",
    "    np.percentile(target_lengths, 95)\n",
    ")) + 2\n",
    "\n",
    "# Criar as camadas de vetorizaÃ§Ã£o SEM limite de vocab (None = ilimitado)\n",
    "source_vectorization = layers.TextVectorization(\n",
    "    max_tokens=None,  # MUDANÃ‡A: Sem limite de vocabulÃ¡rio\n",
    "    output_sequence_length=sequence_length,\n",
    "    standardize=None\n",
    ")\n",
    "\n",
    "target_vectorization = layers.TextVectorization(\n",
    "    max_tokens=None,  # MUDANÃ‡A: Sem limite de vocabulÃ¡rio\n",
    "    output_sequence_length=sequence_length,\n",
    "    standardize=None\n",
    ")\n",
    "\n",
    "# Treinar as camadas nos datasets\n",
    "source_vectorization.adapt(source_phrases_clean)\n",
    "target_vectorization.adapt(target_phrases_final)\n",
    "\n",
    "# CALCULAR vocab_size dinamicamente apÃ³s o adapt()\n",
    "vocab_size_source = len(source_vectorization.get_vocabulary())\n",
    "vocab_size_target = len(target_vectorization.get_vocabulary())\n",
    "\n",
    "print(\"\\nðŸŽ¯ CONFIGURAÃ‡Ã•ES CALCULADAS:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"âœ… Sequence Length: {sequence_length}\")\n",
    "print(f\"âœ… VocabulÃ¡rio PortuguÃªs: {vocab_size_source} tokens\")\n",
    "print(f\"âœ… VocabulÃ¡rio InglÃªs: {vocab_size_target} tokens\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Testar em um exemplo\n",
    "test_phrase_pt = source_phrases_clean[10]\n",
    "vectorized_pt = source_vectorization([test_phrase_pt])\n",
    "\n",
    "test_phrase_en = target_phrases_final[10]\n",
    "vectorized_en = target_vectorization([test_phrase_en])\n",
    "\n",
    "print(f\"\\nðŸ“ TESTE DE VETORIZAÃ‡ÃƒO:\")\n",
    "print(f\"Original Portuguese: '{test_phrase_pt}'\")\n",
    "print(f\"Vectorized: {vectorized_pt.numpy()}\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"Original English: '{test_phrase_en}'\")\n",
    "print(f\"Vectorized: {vectorized_en.numpy()}\")\n",
    "\n",
    "# Verificar tokens especiais\n",
    "pt_vocab = source_vectorization.get_vocabulary()\n",
    "en_vocab = target_vectorization.get_vocabulary()\n",
    "print(f\"\\nðŸ” Primeiras 10 palavras do vocabulÃ¡rio:\")\n",
    "print(f\"PortuguÃªs: {pt_vocab[:10]}\")\n",
    "print(f\"InglÃªs: {en_vocab[:10]}\")\n",
    "print(f\"\\nâœ… '[start]' no vocabulÃ¡rio inglÃªs: {'[start]' in en_vocab}\")\n",
    "print(f\"âœ… '[end]' no vocabulÃ¡rio inglÃªs: {'[end]' in en_vocab}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4642b8e6",
   "metadata": {},
   "source": [
    "## Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4cbb96e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(layers.Layer):\n",
    "    \"\"\"\n",
    "    This layer injects positional information into the input embeddings.\n",
    "    It's a fixed, non-learnable layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, max_length, d_model):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model # Dimension of the embedding vector\n",
    "        self.max_length = max_length # Maximum possible length of a sequence\n",
    "\n",
    "        # Create a positional encoding matrix of shape (max_length, d_model)\n",
    "        # This matrix is pre-calculated and will not change during training.\n",
    "        \n",
    "        # Create a tensor representing positions (0, 1, ..., max_length-1)\n",
    "        # Shape: (max_length, 1)\n",
    "        positions = tf.range(start=0, limit=max_length, delta=1, dtype=tf.float32)\n",
    "        positions = tf.expand_dims(positions, axis=1)\n",
    "\n",
    "        # Calculate the denominator term in the formula.\n",
    "        # 2i/d_model --> [0, 2, 4, ..., d_model-2] / d_model\n",
    "        div_term = tf.exp(tf.range(0, d_model, 2, dtype=tf.float32) * -(np.log(10000.0) / d_model))\n",
    "\n",
    "        # Calculate the angles for the sine and cosine functions\n",
    "        # Broadcasting (positions * div_term) results in a shape of (max_length, d_model/2)\n",
    "        angles = positions * div_term\n",
    "\n",
    "        # Calculate sine for even indices and cosine for odd indices\n",
    "        sin_values = tf.sin(angles)\n",
    "        cos_values = tf.cos(angles)\n",
    "        \n",
    "        # Interleave the sine and cosine values.\n",
    "        # For example, if sin=[s1,s2] and cos=[c1,c2], the result is [s1,c1,s2,c2]\n",
    "        # This creates the final positional encoding matrix.\n",
    "        # Shape: (max_length, d_model)\n",
    "        pe = tf.stack([sin_values, cos_values], axis=2)  # Shape: (max_length, d_model/2, 2)\n",
    "        pe = tf.reshape(pe, [max_length, d_model])       # Shape: (max_length, d_model)\n",
    "        \n",
    "        # Store as TensorFlow constant for better integration\n",
    "        self.positional_encoding = tf.constant(pe, dtype=tf.float32)\n",
    "\n",
    "    def call(self, x):\n",
    "        \"\"\"\n",
    "        The forward pass of the layer.\n",
    "        Args:\n",
    "            x: Input embeddings. Shape: (batch_size, sequence_length, d_model)\n",
    "        Returns:\n",
    "            Embeddings with added positional information.\n",
    "        \"\"\"\n",
    "        # Get the length of the input sequence as a Python int\n",
    "        seq_length = x.shape[1] if x.shape[1] is not None else tf.shape(x)[1]\n",
    "        \n",
    "        # Add the positional encoding to the input embeddings.\n",
    "        # We only use the part of the PE matrix that corresponds to the sequence length.\n",
    "        # Use tf.slice for dynamic slicing which works with both static and dynamic shapes\n",
    "        pos_encoding = tf.slice(self.positional_encoding, [0, 0], [seq_length, self.d_model])\n",
    "        \n",
    "        return x + tf.cast(pos_encoding, dtype=x.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "902bd2f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š ConfiguraÃ§Ã£o do teste:\n",
      "   d_model: 128\n",
      "   max_length: 14\n",
      "   vocab_size (portuguÃªs): 22460\n",
      "   test_seq_len: 10\n",
      "\n",
      "âœ… Shape of word embeddings: (2, 10, 128)\n",
      "âœ… Shape of final embeddings (with positional info): (2, 10, 128)\n"
     ]
    }
   ],
   "source": [
    "# --- Configuration ---\n",
    "# Usar os valores calculados dinamicamente do dataset\n",
    "d_model = 128  # DimensÃ£o dos embeddings\n",
    "max_length = sequence_length  # Usar o sequence_length calculado\n",
    "\n",
    "print(f\"ðŸ“Š ConfiguraÃ§Ã£o do teste:\")\n",
    "print(f\"   d_model: {d_model}\")\n",
    "print(f\"   max_length: {max_length}\")\n",
    "print(f\"   vocab_size (portuguÃªs): {vocab_size_source}\")\n",
    "\n",
    "# --- Create dummy input data ---\n",
    "# CORREÃ‡ÃƒO: O tamanho do dummy_input deve ser <= max_length\n",
    "# Usando max_length-2 para garantir que cabe dentro do limite\n",
    "test_seq_len = min(10, max_length - 1)  # Usa 10 ou max_length-1, o que for menor\n",
    "dummy_input = tf.random.uniform((2, test_seq_len), maxval=vocab_size_source, dtype=tf.int64)\n",
    "\n",
    "print(f\"   test_seq_len: {test_seq_len}\")\n",
    "\n",
    "# --- Build and run the layers ---\n",
    "embedding_layer = layers.Embedding(input_dim=vocab_size_source, output_dim=d_model)\n",
    "positional_encoding_layer = PositionalEncoding(max_length=max_length, d_model=d_model)\n",
    "\n",
    "# 1. Pass input through the embedding layer\n",
    "word_embeddings = embedding_layer(dummy_input)\n",
    "\n",
    "# 2. Add positional information\n",
    "final_embeddings = positional_encoding_layer(word_embeddings)\n",
    "\n",
    "print(f\"\\nâœ… Shape of word embeddings: {word_embeddings.shape}\")\n",
    "print(f\"âœ… Shape of final embeddings (with positional info): {final_embeddings.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbdbfb00",
   "metadata": {},
   "source": [
    "## FunÃ§Ãµes de MÃ¡scara (OtimizaÃ§Ã£o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "53cda80c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§ª Testando funÃ§Ãµes de mÃ¡scara CORRIGIDAS:\n",
      "\n",
      "1. Padding Mask:\n",
      "   Input shape: (2, 4)\n",
      "   Mask shape: (2, 1, 1, 4) â† DEVE SER (2, 1, 1, 4)\n",
      "   Valores Ãºnicos: [-0.e+00 -1.e+09]\n",
      "\n",
      "2. Look-Ahead Mask:\n",
      "   Shape: (5, 5) â† DEVE SER (5, 5)\n",
      "   Valores:\n",
      "[[-0.e+00 -1.e+09 -1.e+09 -1.e+09 -1.e+09]\n",
      " [-0.e+00 -0.e+00 -1.e+09 -1.e+09 -1.e+09]\n",
      " [-0.e+00 -0.e+00 -0.e+00 -1.e+09 -1.e+09]\n",
      " [-0.e+00 -0.e+00 -0.e+00 -0.e+00 -1.e+09]\n",
      " [-0.e+00 -0.e+00 -0.e+00 -0.e+00 -0.e+00]]\n",
      "\n",
      "3. Decoder Masks Combinadas:\n",
      "   Look-ahead mask shape: (1, 1, 5, 5) â† DEVE SER (1, 1, 5, 5)\n",
      "   Cross-attention mask shape: (1, 1, 1, 5) â† DEVE SER (1, 1, 1, 5)\n",
      "\n",
      "âœ… MÃ¡scaras corrigidas com sucesso!\n"
     ]
    }
   ],
   "source": [
    "# Substitua a cÃ©lula de \"FunÃ§Ãµes de MÃ¡scara\" por esta versÃ£o corrigida:\n",
    "\n",
    "def create_padding_mask(seq):\n",
    "    \"\"\"Cria uma mÃ¡scara de adiÃ§Ã£o para zerar a atenÃ§Ã£o nos tokens de padding.\"\"\"\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "    # Adiciona dimensÃµes extras para broadcasting com os scores de atenÃ§Ã£o\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :] * -1e9  # Shape: (batch, 1, 1, seq_len)\n",
    "\n",
    "def create_look_ahead_mask(size):\n",
    "    \"\"\"Cria uma mÃ¡scara de adiÃ§Ã£o para zerar a atenÃ§Ã£o em tokens futuros.\"\"\"\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask * -1e9  # Multiplica por um nÃºmero muito negativo\n",
    "\n",
    "def create_decoder_masks(tar, inp):\n",
    "    \"\"\"Cria todas as mÃ¡scaras de adiÃ§Ã£o necessÃ¡rias para o decoder.\"\"\"\n",
    "    # MÃ¡scara look-ahead para a primeira sub-camada de atenÃ§Ã£o do decoder\n",
    "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
    "\n",
    "    # MÃ¡scara de padding para a primeira sub-camada de atenÃ§Ã£o do decoder\n",
    "    dec_target_padding_mask = create_padding_mask(tar)\n",
    "\n",
    "    # Combina as duas mÃ¡scaras para a self-attention do decoder\n",
    "    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
    "\n",
    "    # MÃ¡scara de padding para a segunda sub-camada (cross-attention), que olha para o encoder\n",
    "    cross_attention_mask = create_padding_mask(inp)\n",
    "\n",
    "    return combined_mask, cross_attention_mask\n",
    "\n",
    "\n",
    "# --- Teste das mÃ¡scaras ---\n",
    "print(\"ðŸ§ª Testando funÃ§Ãµes de mÃ¡scara CORRIGIDAS:\")\n",
    "\n",
    "# Teste 1: Padding mask\n",
    "test_seq = tf.constant([[7, 6, 0, 0], [1, 2, 3, 0]])\n",
    "padding_mask = create_padding_mask(test_seq)\n",
    "print(f\"\\n1. Padding Mask:\")\n",
    "print(f\"   Input shape: {test_seq.shape}\")\n",
    "print(f\"   Mask shape: {padding_mask.shape} â† DEVE SER (2, 1, 1, 4)\")\n",
    "print(f\"   Valores Ãºnicos: {tf.unique(tf.reshape(padding_mask, [-1]))[0].numpy()}\")\n",
    "\n",
    "# Teste 2: Look-ahead mask\n",
    "look_mask = create_look_ahead_mask(5)\n",
    "print(f\"\\n2. Look-Ahead Mask:\")\n",
    "print(f\"   Shape: {look_mask.shape} â† DEVE SER (5, 5)\")\n",
    "print(f\"   Valores:\\n{look_mask.numpy()}\")\n",
    "\n",
    "# Teste 3: MÃ¡scaras combinadas do decoder\n",
    "test_tar = tf.constant([[1, 2, 3, 0, 0]])\n",
    "test_inp = tf.constant([[4, 5, 6, 7, 0]])\n",
    "look_ahead_combined, padding_combined = create_decoder_masks(test_tar, test_inp)\n",
    "print(f\"\\n3. Decoder Masks Combinadas:\")\n",
    "print(f\"   Look-ahead mask shape: {look_ahead_combined.shape} â† DEVE SER (1, 1, 5, 5)\")\n",
    "print(f\"   Cross-attention mask shape: {padding_combined.shape} â† DEVE SER (1, 1, 1, 5)\")\n",
    "\n",
    "print(\"\\nâœ… MÃ¡scaras corrigidas com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab0efcd",
   "metadata": {},
   "source": [
    "## Self-Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3dd8de95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the input: (2, 15, 128)\n",
      "Shape of the output (from Keras MHA): (2, 15, 128)\n"
     ]
    }
   ],
   "source": [
    "# --- Configuration ---\n",
    "d_model = 128\n",
    "num_heads = 8\n",
    "\n",
    "# The dimension of each head is d_model / num_heads\n",
    "key_dim = d_model // num_heads\n",
    "\n",
    "# --- Create dummy input data ---\n",
    "# Batch of 2 sentences, length 15, embedding dim 128\n",
    "dummy_input = tf.random.uniform((2, 15, d_model))\n",
    "\n",
    "# --- Build and run the layer ---\n",
    "mha_layer = layers.MultiHeadAttention(\n",
    "    num_heads=num_heads,\n",
    "    key_dim=key_dim,\n",
    "    output_shape=d_model # Ensures the output dimension is correct\n",
    ")\n",
    "\n",
    "# In self-attention, query, value, and key are the same.\n",
    "output_keras = mha_layer(query=dummy_input, value=dummy_input, key=dummy_input)\n",
    "\n",
    "print(\"Shape of the input:\", dummy_input.shape)\n",
    "print(\"Shape of the output (from Keras MHA):\", output_keras.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2dd5c44",
   "metadata": {},
   "source": [
    "## Encoder Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e01bea65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š ConfiguraÃ§Ã£o do Encoder Block:\n",
      "   d_model: 128\n",
      "   num_heads: 8\n",
      "   dff: 128 (otimizado para velocidade)\n",
      "   dropout_rate: 0.1\n",
      "\n",
      "âœ… Shape of the input to the Encoder Block: (2, 15, 128)\n",
      "âœ… Shape of the output from the Encoder Block: (2, 15, 128)\n"
     ]
    }
   ],
   "source": [
    "# --- Encoder Block Configuration ---\n",
    "d_model = 128       # Dimension of the model (embedding size)\n",
    "num_heads = 8       # Number of attention heads\n",
    "dff = 128           # OTIMIZAÃ‡ÃƒO: Reduzido de 512 para 128 (igual ao exemplo Kaggle)\n",
    "dropout_rate = 0.1  # Dropout rate for regularization\n",
    "\n",
    "print(f\"ðŸ“Š ConfiguraÃ§Ã£o do Encoder Block:\")\n",
    "print(f\"   d_model: {d_model}\")\n",
    "print(f\"   num_heads: {num_heads}\")\n",
    "print(f\"   dff: {dff} (otimizado para velocidade)\")\n",
    "print(f\"   dropout_rate: {dropout_rate}\")\n",
    "\n",
    "class EncoderBlock(layers.Layer):\n",
    "    \"\"\"\n",
    "    Represents one block of the Transformer's Encoder.\n",
    "    It consists of Multi-Head Attention and a Feed-Forward Network,\n",
    "    with residual connections and layer normalization.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.supports_masking = True\n",
    "\n",
    "        # Multi-Head Attention Layer\n",
    "        self.mha = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim=d_model // num_heads,\n",
    "            output_shape=d_model\n",
    "        )\n",
    "\n",
    "        # Feed-Forward Network (consists of two dense layers)\n",
    "        self.ffn = keras.Sequential([\n",
    "            layers.Dense(dff, activation='relu'), # (batch_size, seq_len, dff)\n",
    "            layers.Dense(d_model)                 # (batch_size, seq_len, d_model)\n",
    "        ])\n",
    "\n",
    "        # Layer Normalization\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        # Dropout for regularization\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, training, padding_mask=None):\n",
    "        # 1. Multi-Head Attention sub-layer\n",
    "        # The input 'x' is used for query, key, and value in self-attention\n",
    "        attn_output = self.mha(query=x, value=x, key=x, attention_mask=padding_mask)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        # Residual connection and Layer Normalization\n",
    "        out1 = self.layernorm1(x + attn_output)\n",
    "\n",
    "        # 2. Feed-Forward Network sub-layer\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        # Residual connection and Layer Normalization\n",
    "        out2 = self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "        return out2\n",
    "\n",
    "# --- Test the EncoderBlock ---\n",
    "encoder_block = EncoderBlock(d_model=d_model, num_heads=num_heads, dff=dff)\n",
    "\n",
    "# Create some dummy input (output from positional encoding)\n",
    "dummy_input = tf.random.uniform((2, 15, d_model)) # (batch_size, sequence_length, d_model)\n",
    "\n",
    "# The call method requires a 'training' flag for dropout\n",
    "output_from_block = encoder_block(dummy_input, training=False)\n",
    "\n",
    "print(f\"\\nâœ… Shape of the input to the Encoder Block: {dummy_input.shape}\")\n",
    "print(f\"âœ… Shape of the output from the Encoder Block: {output_from_block.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ccd20f",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "65488aea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š ConfiguraÃ§Ã£o do Encoder completo:\n",
      "   num_layers: 2\n",
      "   d_model: 128\n",
      "   num_heads: 8\n",
      "   dff: 128\n",
      "   input_vocab_size: 22460\n",
      "   max_sentence_length: 14\n",
      "\n",
      "âœ… Shape of the input to the Encoder (vectorized text): (2, 14)\n",
      "âœ… Shape of the output from the Encoder (context vectors): (2, 14, 128)\n"
     ]
    }
   ],
   "source": [
    "class Encoder(layers.Layer):\n",
    "    \"\"\"\n",
    "    The complete Encoder, consisting of an embedding layer, positional encoding,\n",
    "    and a stack of N EncoderBlocks.\n",
    "    \n",
    "    OTIMIZAÃ‡ÃƒO: Agora usa mÃ¡scaras de padding corretamente!\n",
    "    \"\"\"\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff,\n",
    "                 input_vocab_size, max_length, rate=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.supports_masking = True\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Input Embedding layer\n",
    "        self.embedding = layers.Embedding(input_vocab_size, d_model)\n",
    "        \n",
    "        # Positional Encoding layer\n",
    "        self.pos_encoding = PositionalEncoding(max_length, self.d_model)\n",
    "\n",
    "        # Stack of EncoderBlocks\n",
    "        # We use a list comprehension to create N encoder blocks\n",
    "        self.enc_layers = [EncoderBlock(d_model, num_heads, dff, rate)\n",
    "                           for _ in range(num_layers)]\n",
    "\n",
    "        # Dropout layer for regularization\n",
    "        self.dropout = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, training=False, padding_mask=None):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "\n",
    "        # 1. Get embeddings\n",
    "        x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
    "        \n",
    "        # A scaling factor is applied to the embeddings as in the original paper\n",
    "        # CORREÃ‡ÃƒO: Converter para o mesmo dtype dos embeddings (compatÃ­vel com mixed precision)\n",
    "        x *= tf.cast(tf.math.sqrt(tf.cast(self.d_model, tf.float32)), x.dtype)\n",
    "\n",
    "        # 2. Add positional encoding\n",
    "        x = self.pos_encoding(x)\n",
    "\n",
    "        # 3. Apply dropout\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        # 4. Pass through the stack of encoder blocks\n",
    "        # OTIMIZAÃ‡ÃƒO: Agora passa a mÃ¡scara para cada bloco\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x, training=training, padding_mask=padding_mask)\n",
    "\n",
    "        return x  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "# --- Test the complete Encoder ---\n",
    "\n",
    "# OTIMIZAÃ‡ÃƒO: Usar valores calculados dinamicamente do dataset\n",
    "num_layers_encoder = 2  # Stack 2 blocks for this test\n",
    "d_model = 128\n",
    "num_heads = 8\n",
    "dff = 128  # OTIMIZADO: Reduzido para 128\n",
    "input_vocab_size = vocab_size_source  # CALCULADO DINAMICAMENTE\n",
    "max_sentence_length = sequence_length  # CALCULADO DINAMICAMENTE\n",
    "\n",
    "print(f\"ðŸ“Š ConfiguraÃ§Ã£o do Encoder completo:\")\n",
    "print(f\"   num_layers: {num_layers_encoder}\")\n",
    "print(f\"   d_model: {d_model}\")\n",
    "print(f\"   num_heads: {num_heads}\")\n",
    "print(f\"   dff: {dff}\")\n",
    "print(f\"   input_vocab_size: {input_vocab_size}\")\n",
    "print(f\"   max_sentence_length: {max_sentence_length}\")\n",
    "\n",
    "# Create an instance of the Encoder\n",
    "encoder = Encoder(num_layers=num_layers_encoder,\n",
    "                  d_model=d_model,\n",
    "                  num_heads=num_heads,\n",
    "                  dff=dff,\n",
    "                  input_vocab_size=input_vocab_size,\n",
    "                  max_length=max_sentence_length)\n",
    "\n",
    "# Create some dummy input (vectorized portuguese sentences)\n",
    "dummy_pt_vector = source_vectorization([\"eu amo pizza\", \"onde fica o banheiro\"])\n",
    "\n",
    "# Get the output from the encoder\n",
    "encoder_output = encoder(dummy_pt_vector, training=False)\n",
    "\n",
    "print(f\"\\nâœ… Shape of the input to the Encoder (vectorized text): {dummy_pt_vector.shape}\")\n",
    "print(f\"âœ… Shape of the output from the Encoder (context vectors): {encoder_output.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e385314",
   "metadata": {},
   "source": [
    "## Decoder Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d2d25187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š ConfiguraÃ§Ã£o do Decoder Block:\n",
      "   d_model: 128\n",
      "   num_heads: 8\n",
      "   dff: 128 (otimizado)\n",
      "\n",
      "âœ… Shape of the decoder input: (2, 15, 128)\n",
      "âœ… Shape of the encoder output: (2, 20, 128)\n",
      "âœ… Shape of the output from the Decoder Block: (2, 15, 128)\n"
     ]
    }
   ],
   "source": [
    "# This code builds on the previous cells in your notebook.\n",
    "# Make sure you have run the cells defining d_model, num_heads, dff etc.\n",
    "\n",
    "class DecoderBlock(layers.Layer):\n",
    "    \"\"\"\n",
    "    Represents one block of the Transformer's Decoder.\n",
    "    It consists of Masked Multi-Head Self-Attention, Cross-Attention,\n",
    "    and a Feed-Forward Network.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.supports_masking = True\n",
    "\n",
    "        # 1. Masked Multi-Head Self-Attention (looks at the target sequence)\n",
    "        self.mha1 = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim=d_model // num_heads,\n",
    "            output_shape=d_model\n",
    "        )\n",
    "\n",
    "        # 2. Multi-Head Cross-Attention (looks at the encoder output)\n",
    "        self.mha2 = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim=d_model // num_heads,\n",
    "            output_shape=d_model\n",
    "        )\n",
    "\n",
    "        # 3. Feed-Forward Network\n",
    "        self.ffn = keras.Sequential([\n",
    "            layers.Dense(dff, activation='relu'),\n",
    "            layers.Dense(d_model)\n",
    "        ])\n",
    "\n",
    "        # Layer Normalization layers\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        # Dropout layers\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "        self.dropout3 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, encoder_output, training, look_ahead_mask, cross_attention_mask):\n",
    "        # Sub-layer 1: Masked Multi-Head Self-Attention\n",
    "        # The decoder attends to itself, but can't see future tokens.\n",
    "        attn1 = self.mha1(query=x, value=x, key=x, attention_mask=look_ahead_mask)\n",
    "        attn1 = self.dropout1(attn1, training=training)\n",
    "        out1 = self.layernorm1(x + attn1)\n",
    "\n",
    "        # Sub-layer 2: Cross-Attention\n",
    "        # The decoder attends to the encoder's output.\n",
    "        # Query comes from the decoder, Key and Value from the encoder.\n",
    "        # Usa a mÃ¡scara de padding vinda do encoder.\n",
    "        attn2 = self.mha2(query=out1, value=encoder_output, key=encoder_output, attention_mask=cross_attention_mask)\n",
    "        attn2 = self.dropout2(attn2, training=training)\n",
    "        out2 = self.layernorm2(out1 + attn2)\n",
    "\n",
    "        # Sub-layer 3: Feed-Forward Network\n",
    "        ffn_output = self.ffn(out2)\n",
    "        ffn_output = self.dropout3(ffn_output, training=training)\n",
    "        out3 = self.layernorm3(out2 + ffn_output)\n",
    "\n",
    "        return out3\n",
    "\n",
    "# --- Test the DecoderBlock ---\n",
    "# OTIMIZAÃ‡ÃƒO: Usar valores otimizados\n",
    "d_model = 128\n",
    "num_heads = 8\n",
    "dff = 128  # OTIMIZADO: Reduzido para 128\n",
    "\n",
    "print(f\"ðŸ“Š ConfiguraÃ§Ã£o do Decoder Block:\")\n",
    "print(f\"   d_model: {d_model}\")\n",
    "print(f\"   num_heads: {num_heads}\")\n",
    "print(f\"   dff: {dff} (otimizado)\")\n",
    "\n",
    "decoder_block = DecoderBlock(d_model=d_model, num_heads=num_heads, dff=dff)\n",
    "\n",
    "# Create dummy inputs\n",
    "# Note: The sequence lengths for the decoder input and encoder output can be different.\n",
    "dummy_decoder_input = tf.random.uniform((2, 15, d_model)) # (batch_size, target_seq_len, d_model)\n",
    "dummy_encoder_output = tf.random.uniform((2, 20, d_model)) # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "# For testing shapes, we can pass masks as None\n",
    "output_from_block = decoder_block(\n",
    "    dummy_decoder_input,\n",
    "    dummy_encoder_output,\n",
    "    training=False,\n",
    "    look_ahead_mask=None,\n",
    "    cross_attention_mask=None\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ… Shape of the decoder input: {dummy_decoder_input.shape}\")\n",
    "print(f\"âœ… Shape of the encoder output: {dummy_encoder_output.shape}\")\n",
    "print(f\"âœ… Shape of the output from the Decoder Block: {output_from_block.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11de9328",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2928128c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š ConfiguraÃ§Ã£o do Decoder completo:\n",
      "   num_layers: 2\n",
      "   d_model: 128\n",
      "   num_heads: 8\n",
      "   dff: 128\n",
      "   target_vocab_size: 12908\n",
      "   max_sentence_length: 14\n",
      "\n",
      "âœ… Shape of the decoder input (vectorized English text): (2, 14)\n",
      "âœ… Shape of the encoder output (context vectors): (2, 14, 128)\n",
      "âœ… Shape of the final decoder output: (2, 14, 128)\n"
     ]
    }
   ],
   "source": [
    "class Decoder(layers.Layer):\n",
    "    \"\"\"\n",
    "    The complete Decoder, consisting of an embedding layer, positional encoding,\n",
    "    and a stack of N DecoderBlocks.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff,\n",
    "                 target_vocab_size, max_length, rate=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.supports_masking = True\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Input Embedding layer for the target language\n",
    "        self.embedding = layers.Embedding(target_vocab_size, d_model)\n",
    "\n",
    "        # Positional Encoding layer\n",
    "        self.pos_encoding = PositionalEncoding(max_length, d_model)\n",
    "\n",
    "        # Stack of DecoderBlocks\n",
    "        self.dec_layers = [DecoderBlock(d_model, num_heads, dff, rate)\n",
    "                           for _ in range(num_layers)]\n",
    "\n",
    "        # Dropout layer\n",
    "        self.dropout = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, encoder_output, training=False, look_ahead_mask=None, cross_attention_mask=None):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "\n",
    "        # 1. Get embeddings for the target sequence\n",
    "        x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
    "        # CORREÃ‡ÃƒO: Converter para o mesmo dtype dos embeddings (compatÃ­vel com mixed precision)\n",
    "        x *= tf.cast(tf.math.sqrt(tf.cast(self.d_model, tf.float32)), x.dtype)\n",
    "\n",
    "        # 2. Add positional encoding\n",
    "        x = self.pos_encoding(x)\n",
    "\n",
    "        # 3. Apply dropout\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        # 4. Pass through the stack of decoder blocks\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.dec_layers[i](x, encoder_output, training=training,\n",
    "                                     look_ahead_mask=look_ahead_mask,\n",
    "                                     cross_attention_mask=cross_attention_mask)\n",
    "\n",
    "        # The shape of x is (batch_size, target_seq_len, d_model)\n",
    "        return x\n",
    "\n",
    "# --- Test the complete Decoder ---\n",
    "\n",
    "# OTIMIZAÃ‡ÃƒO: Usar valores calculados dinamicamente\n",
    "num_layers_decoder = 2  # Stack 2 blocks\n",
    "d_model = 128\n",
    "num_heads = 8\n",
    "dff = 128  # OTIMIZADO: Reduzido para 128\n",
    "target_vocab_size = vocab_size_target  # CALCULADO DINAMICAMENTE\n",
    "max_sentence_length = sequence_length  # CALCULADO DINAMICAMENTE\n",
    "\n",
    "print(f\"ðŸ“Š ConfiguraÃ§Ã£o do Decoder completo:\")\n",
    "print(f\"   num_layers: {num_layers_decoder}\")\n",
    "print(f\"   d_model: {d_model}\")\n",
    "print(f\"   num_heads: {num_heads}\")\n",
    "print(f\"   dff: {dff}\")\n",
    "print(f\"   target_vocab_size: {target_vocab_size}\")\n",
    "print(f\"   max_sentence_length: {max_sentence_length}\")\n",
    "\n",
    "# Create an instance of the Decoder\n",
    "decoder = Decoder(num_layers=num_layers_decoder,\n",
    "                  d_model=d_model,\n",
    "                  num_heads=num_heads,\n",
    "                  dff=dff,\n",
    "                  target_vocab_size=target_vocab_size,\n",
    "                  max_length=max_sentence_length)\n",
    "\n",
    "# Create dummy inputs\n",
    "# We need both a dummy decoder input and the dummy encoder output from the previous test\n",
    "dummy_en_vector = target_vectorization([\"[start] i love pizza\", \"[start] where is the bathroom\"])\n",
    "# We can reuse the encoder_output from the Encoder test cell\n",
    "# encoder_output has shape (2, sequence_length, 128)\n",
    "\n",
    "# Get the output from the decoder\n",
    "decoder_output = decoder(\n",
    "    x=dummy_en_vector,\n",
    "    encoder_output=encoder_output, # This comes from the Encoder\n",
    "    training=False,\n",
    "    look_ahead_mask=None # Passing None for shape testing\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ… Shape of the decoder input (vectorized English text): {dummy_en_vector.shape}\")\n",
    "print(f\"âœ… Shape of the encoder output (context vectors): {encoder_output.shape}\")\n",
    "print(f\"âœ… Shape of the final decoder output: {decoder_output.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c86c200",
   "metadata": {},
   "source": [
    "## Modelo Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "039ef33c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ðŸš€ CRIANDO MODELO TRANSFORMER COM CONFIGURAÃ‡Ã•ES OTIMIZADAS\n",
      "======================================================================\n",
      "   num_layers          : 2\n",
      "   d_model             : 128\n",
      "   num_heads           : 8\n",
      "   dff                 : 128\n",
      "   input_vocab_size    : 22460\n",
      "   target_vocab_size   : 12908\n",
      "   max_length          : 200\n",
      "======================================================================\n",
      "\n",
      "âœ… Shape of Portuguese input: (2, 14)\n",
      "âœ… Shape of English input: (2, 14)\n",
      "âœ… Shape of the final Transformer output (logits): (2, 14, 12908)\n",
      "\n",
      "ðŸŽ‰ Modelo Transformer criado com sucesso!\n"
     ]
    }
   ],
   "source": [
    "class Transformer(keras.Model):\n",
    "    \"\"\"\n",
    "    The complete Transformer model, connecting the Encoder and Decoder.\n",
    "    \n",
    "    OTIMIZAÃ‡ÃƒO: Agora cria e usa mÃ¡scaras de padding corretamente!\n",
    "    Isso melhora a acurÃ¡cia e acelera a convergÃªncia.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff,\n",
    "                 input_vocab_size, target_vocab_size, max_length, rate=0.1):\n",
    "        super().__init__()\n",
    "        # Instantiate the Encoder\n",
    "        self.encoder = Encoder(num_layers, d_model, num_heads, dff,\n",
    "                               input_vocab_size, max_length, rate)\n",
    "\n",
    "        # Instantiate the Decoder\n",
    "        self.decoder = Decoder(num_layers, d_model, num_heads, dff,\n",
    "                               target_vocab_size, max_length, rate)\n",
    "\n",
    "        # The final linear layer to project decoder output to vocabulary size\n",
    "        # OTIMIZAÃ‡ÃƒO: dtype='float32' para compatibilidade com mixed precision\n",
    "        self.final_layer = layers.Dense(target_vocab_size, dtype='float32')\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        # The 'inputs' will be a tuple: (source_sequence, target_sequence)\n",
    "        inp, tar = inputs\n",
    "\n",
    "        # Criar mÃ¡scaras de padding\n",
    "        enc_padding_mask = create_padding_mask(inp)\n",
    "        \n",
    "        # Criar as duas mÃ¡scaras para o decoder\n",
    "        look_ahead_mask, cross_attention_mask = create_decoder_masks(tar, inp)\n",
    "\n",
    "        # 1. Pass the source sequence through the encoder\n",
    "        encoder_output = self.encoder(inp, training=training, padding_mask=enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n",
    "\n",
    "        # 2. Pass the encoder output and the target sequence through the decoder\n",
    "        decoder_output = self.decoder(\n",
    "            tar, encoder_output, training=training, \n",
    "            look_ahead_mask=look_ahead_mask,\n",
    "            cross_attention_mask=cross_attention_mask\n",
    "        ) # (batch_size, tar_seq_len, d_model)\n",
    "\n",
    "        # 3. Pass the decoder output through the final linear layer\n",
    "        final_output = self.final_layer(decoder_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
    "\n",
    "        return final_output\n",
    "\n",
    "# --- Test the complete Transformer model ---\n",
    "\n",
    "# OTIMIZAÃ‡ÃƒO: Usar configuraÃ§Ãµes calculadas dinamicamente do dataset\n",
    "print(\"=\" * 70)\n",
    "print(\"ðŸš€ CRIANDO MODELO TRANSFORMER COM CONFIGURAÃ‡Ã•ES OTIMIZADAS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "PE_MAX_LENGTH = 200\n",
    "\n",
    "config_summary = {\n",
    "    \"num_layers\": num_layers_encoder,\n",
    "    \"d_model\": d_model,\n",
    "    \"num_heads\": num_heads,\n",
    "    \"dff\": dff,\n",
    "    \"input_vocab_size\": vocab_size_source,\n",
    "    \"target_vocab_size\": vocab_size_target,\n",
    "    \"max_length\": PE_MAX_LENGTH\n",
    "}\n",
    "\n",
    "for key, value in config_summary.items():\n",
    "    print(f\"   {key:20s}: {value}\")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "\n",
    "transformer = Transformer(\n",
    "    num_layers=num_layers_encoder,\n",
    "    d_model=d_model,\n",
    "    num_heads=num_heads,\n",
    "    dff=dff,\n",
    "    input_vocab_size=vocab_size_source,\n",
    "    target_vocab_size=vocab_size_target,\n",
    "    max_length=PE_MAX_LENGTH\n",
    ")\n",
    "\n",
    "# Reuse the dummy vectors from previous tests\n",
    "# The call method expects a tuple of (input, target)\n",
    "output = transformer((dummy_pt_vector, dummy_en_vector), training=False)\n",
    "\n",
    "print(f\"\\nâœ… Shape of Portuguese input: {dummy_pt_vector.shape}\")\n",
    "print(f\"âœ… Shape of English input: {dummy_en_vector.shape}\")\n",
    "print(f\"âœ… Shape of the final Transformer output (logits): {output.shape}\")\n",
    "print(f\"\\nðŸŽ‰ Modelo Transformer criado com sucesso!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151ab3b8",
   "metadata": {},
   "source": [
    "## FunÃ§Ã£o de Perda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "cc4a24d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to define our loss function and metrics\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none' # 'none' means we get a loss for each example\n",
    ")\n",
    "\n",
    "def masked_loss_function(real, pred):\n",
    "    # Create a mask to identify where the real target is not a padding token (0)\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    \n",
    "    # Calculate the loss for every token in the batch\n",
    "    loss_ = loss_object(real, pred)\n",
    "    \n",
    "    # Cast the mask to the same type as the loss\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    \n",
    "    # Apply the mask to the loss, effectively zeroing out the loss for padding tokens\n",
    "    loss_ *= mask\n",
    "    \n",
    "    # Return the average loss over the non-padded tokens\n",
    "    return tf.reduce_sum(loss_) / tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a55696",
   "metadata": {},
   "source": [
    "## Learning Rate Schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0bfa97f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super().__init__()\n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        step = tf.cast(step, tf.float32)\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "    \n",
    "    def get_config(self):\n",
    "        \"\"\"Retorna a configuraÃ§Ã£o para serializaÃ§Ã£o.\"\"\"\n",
    "        return {\n",
    "            \"d_model\": int(self.d_model.numpy()),\n",
    "            \"warmup_steps\": self.warmup_steps\n",
    "        }\n",
    "\n",
    "# Instantiate the learning rate schedule and the Adam optimizer\n",
    "learning_rate = CustomSchedule(d_model)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516f056c",
   "metadata": {},
   "source": [
    "## Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "badbbdc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Prepare the Data for Training and Validation ---\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Ensure eager execution is enabled\n",
    "#tf.config.run_functions_eagerly(True)\n",
    "\n",
    "# OTIMIZAÃ‡ÃƒO: Aumentar batch size para acelerar treinamento\n",
    "# Batch maior = menos steps por Ã©poca = treinamento mais rÃ¡pido\n",
    "# Valor anterior: 64 | Novo valor: 256 (4x mais rÃ¡pido)\n",
    "batch_size = 512  # Ajuste para 128 se tiver problemas de memÃ³ria\n",
    "\n",
    "# Vectorize all the sentences\n",
    "source_vectors = source_vectorization(source_phrases_clean)\n",
    "target_vectors = target_vectorization(target_phrases_final)\n",
    "\n",
    "# Convert to numpy arrays for sklearn compatibility\n",
    "source_vectors_np = source_vectors.numpy()\n",
    "target_vectors_np = target_vectors.numpy()\n",
    "\n",
    "# Split the data into training and validation sets (80/20 split)\n",
    "source_train, source_val, target_train, target_val = train_test_split(\n",
    "    source_vectors_np, target_vectors_np, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Total examples: {len(source_vectors_np)}\")\n",
    "print(f\"Training examples: {len(source_train)}\")\n",
    "print(f\"Validation examples: {len(source_val)}\")\n",
    "\n",
    "# Helper function to create datasets\n",
    "def create_dataset(source, target):\n",
    "    \"\"\"Helper function to create a tf.data.Dataset with shifted targets.\"\"\"\n",
    "    # The input to the decoder is the target sequence without the last token\n",
    "    decoder_inputs = target[:, :-1]\n",
    "    # The target for the loss function is the target sequence without the first token\n",
    "    decoder_outputs = target[:, 1:]\n",
    "    \n",
    "    # Create the tf.data.Dataset object\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(\n",
    "        ((source, decoder_inputs), decoder_outputs)\n",
    "    )\n",
    "    # Shuffle, batch, and prefetch\n",
    "    return dataset.shuffle(buffer_size=len(source)).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Create the training and validation datasets\n",
    "train_dataset = create_dataset(source_train, target_train)\n",
    "val_dataset = create_dataset(source_val, target_val)\n",
    "\n",
    "# --- 2. Compile the Model ---\n",
    "\n",
    "# We need a metric to monitor performance, let's create a masked accuracy\n",
    "def masked_accuracy(real, pred):\n",
    "    # Get the predicted word by finding the index with the highest logit\n",
    "    pred = tf.argmax(pred, axis=2)\n",
    "    real = tf.cast(real, pred.dtype)\n",
    "    \n",
    "    # Check for matches\n",
    "    match = tf.cast(real == pred, dtype=tf.float32)\n",
    "    \n",
    "    # Create the padding mask\n",
    "    mask = tf.cast(tf.math.logical_not(tf.math.equal(real, 0)), dtype=tf.float32)\n",
    "    \n",
    "    # Apply the mask to the matches\n",
    "    match *= mask\n",
    "    \n",
    "    # Calculate the accuracy on non-padded tokens\n",
    "    return tf.reduce_sum(match) / tf.reduce_sum(mask)\n",
    "\n",
    "# Compile the model with the optimizer, loss, and metric\n",
    "transformer.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=masked_loss_function,\n",
    "    metrics=[masked_accuracy]\n",
    "    #run_eagerly=True  # Run in eager mode to avoid graph mode issues\n",
    ")\n",
    "\n",
    "# --- 3. Configure Early Stopping ---\n",
    "\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Early Stopping: para o treinamento se val_loss nÃ£o melhorar por 'patience' Ã©pocas\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',           # Monitora a perda de validaÃ§Ã£o\n",
    "    patience=2,                   # Espera 2 Ã©pocas sem melhora antes de parar\n",
    "    restore_best_weights=True,    # Restaura os pesos da melhor Ã©poca\n",
    "    verbose=1,                    # Mostra mensagens quando parar\n",
    "    mode='min'                    # 'min' porque queremos minimizar a loss\n",
    ")\n",
    "\n",
    "# OTIMIZAÃ‡ÃƒO: Model Checkpoint - salva o melhor modelo automaticamente\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    filepath='best_transformer_model.keras',  # Nome do arquivo\n",
    "    monitor='val_loss',                       # MÃ©trica a monitorar\n",
    "    save_best_only=True,                      # Salva apenas quando melhorar\n",
    "    save_weights_only=False,                  # Salva modelo completo\n",
    "    mode='min',                               # Minimizar a loss\n",
    "    verbose=1                                 # Mostrar quando salvar\n",
    ")\n",
    "\n",
    "print(\"âœ… Callbacks configurados:\")\n",
    "print(f\"   - Early Stopping (patience={early_stopping.patience})\")\n",
    "print(\"   - Model Checkpoint (salva em 'best_transformer_model.keras')\")\n",
    "\n",
    "# --- 4. Train the Model with Validation and Early Stopping ---\n",
    "\n",
    "print(\"Starting training with validation and Early Stopping...\")\n",
    "print(f\"Training will stop automatically if validation loss doesn't improve for {early_stopping.patience} epochs.\")\n",
    "print(\"=\" * 80)\n",
    "print(\"ðŸš€ OTIMIZAÃ‡Ã•ES ATIVAS:\")\n",
    "print(\"   âœ… Mixed Precision (float16) - Acelera cÃ¡lculos\")\n",
    "print(\"   âœ… MÃ¡scaras de Padding - Melhora acurÃ¡cia e convergÃªncia\")\n",
    "print(f\"   âœ… Batch Size: {batch_size} - Acelera treinamento\")\n",
    "print(\"   âœ… Model Checkpoint - Salva melhor modelo automaticamente\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Definimos mais Ã©pocas, mas o early stopping pode parar antes\n",
    "epochs = 5 # 100\n",
    "\n",
    "history = transformer.fit(\n",
    "    train_dataset,\n",
    "    epochs=epochs,\n",
    "    validation_data=val_dataset,\n",
    "    callbacks=[early_stopping, model_checkpoint]  # OTIMIZAÃ‡ÃƒO: Adiciona checkpoint\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Training finished!\")\n",
    "print(f\"Total epochs trained: {len(history.history['loss'])}\")\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e693fc5",
   "metadata": {},
   "source": [
    "## Carregando o Melhor Modelo (Opcional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ac91bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se vocÃª quiser carregar o melhor modelo salvo pelo checkpoint:\n",
    "# (Ãštil se vocÃª reiniciar o kernel ou quiser usar o modelo em outra sessÃ£o)\n",
    "\n",
    "# Para carregar o modelo completo:\n",
    "# transformer = keras.models.load_model('best_transformer_model.keras')\n",
    "\n",
    "# OU para apenas verificar se o arquivo existe:\n",
    "# import os\n",
    "# if os.path.exists('best_transformer_model.keras'):\n",
    "#     print(\"âœ… Melhor modelo salvo em: best_transformer_model.keras\")\n",
    "#     print(f\"   Tamanho do arquivo: {os.path.getsize('best_transformer_model.keras') / 1024 / 1024:.2f} MB\")\n",
    "# else:\n",
    "#     print(\"âš ï¸ Modelo ainda nÃ£o foi salvo (treine primeiro)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7373d1fa",
   "metadata": {},
   "source": [
    "## VisualizaÃ§Ã£o das Curvas de Aprendizagem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a0a222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the training history\n",
    "train_acc = history.history['masked_accuracy']\n",
    "val_acc = history.history['val_masked_accuracy']\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "# Create the epochs range\n",
    "epochs_range = range(len(train_acc))\n",
    "\n",
    "# Plot the training and validation curves\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# --- Plot Accuracy ---\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, train_acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True)\n",
    "\n",
    "# --- Plot Loss ---\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, train_loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nFinal Training Loss: {train_loss[-1]:.4f}\")\n",
    "print(f\"Final Validation Loss: {val_loss[-1]:.4f}\")\n",
    "print(f\"Final Training Accuracy: {train_acc[-1]:.4f}\")\n",
    "print(f\"Final Validation Accuracy: {val_acc[-1]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0005e661",
   "metadata": {},
   "source": [
    "## InferÃªncia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4fe42c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the target vocabulary\n",
    "target_vocab = target_vectorization.get_vocabulary()\n",
    "\n",
    "# Create a dictionary to map the target language index back to a word\n",
    "index_to_word = {i: word for i, word in enumerate(target_vocab)}\n",
    "\n",
    "# Define the maximum length for a generated translation\n",
    "max_output_length = 20\n",
    "\n",
    "# Find the indices for start and end tokens (with brackets preserved)\n",
    "start_token_index = target_vocab.index('[start]')\n",
    "end_token_index = target_vocab.index('[end]')\n",
    "\n",
    "print(f\"Token '[start]' tem Ã­ndice: {start_token_index}\")\n",
    "print(f\"Token '[end]' tem Ã­ndice: {end_token_index}\")\n",
    "\n",
    "def translate(sentence, debug=False):\n",
    "    \"\"\"\n",
    "    Translates a Portuguese sentence to English using the trained Transformer model.\n",
    "    \"\"\"\n",
    "    # 1. Preprocess the input sentence\n",
    "    cleaned_sentence = standardize_text(sentence)\n",
    "    \n",
    "    # 2. Vectorize the sentence and add a batch dimension\n",
    "    input_vector = source_vectorization([cleaned_sentence]) # Shape: (1, sequence_length)\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"  Input vectorizado: {input_vector.numpy()}\")\n",
    "    \n",
    "    # 3. Initialize the decoder's input with the '[start]' token\n",
    "    # The decoder input starts as a tensor with the start token index\n",
    "    decoder_input = tf.constant([[start_token_index]], dtype=tf.int64)\n",
    "\n",
    "    generated_tokens = []\n",
    "    for i in range(max_output_length):\n",
    "        # 4. Get the model's predictions\n",
    "        # The model is called in a non-training mode\n",
    "        predictions = transformer((input_vector, decoder_input), training=False)\n",
    "        \n",
    "        # 5. Get the logits for the very last predicted token\n",
    "        # predictions shape: (batch_size, seq_len, vocab_size) -> (1, i+1, vocab_size)\n",
    "        # We want the predictions for the last token in the sequence\n",
    "        last_token_logits = predictions[:, -1, :] # Shape: (1, vocab_size)\n",
    "        \n",
    "        # 6. Find the token with the highest probability (greedy search)\n",
    "        predicted_id = tf.argmax(last_token_logits, axis=-1) # Shape: (1,)\n",
    "        predicted_id_value = int(predicted_id.numpy()[0])\n",
    "        \n",
    "        if debug:\n",
    "            print(f\"  Step {i}: Predicted token ID = {predicted_id_value}, word = '{index_to_word.get(predicted_id_value, '???')}'\")\n",
    "        \n",
    "        generated_tokens.append(predicted_id_value)\n",
    "        \n",
    "        # 7. Add the predicted token to the decoder input for the next iteration\n",
    "        decoder_input = tf.concat([decoder_input, [predicted_id]], axis=1)\n",
    "        \n",
    "        # 8. Check if the predicted token is the '[end]' token\n",
    "        if predicted_id_value == end_token_index:\n",
    "            if debug:\n",
    "                print(f\"  Token '[end]' found, stopping generation.\")\n",
    "            break\n",
    "            \n",
    "    # 9. Convert the sequence of token IDs back to words\n",
    "    # We ignore the [start] and [end] tokens\n",
    "    output_tokens = [token for token in generated_tokens if token != end_token_index]\n",
    "    translated_text = \" \".join(index_to_word[token] for token in output_tokens)\n",
    "    \n",
    "    return translated_text\n",
    "\n",
    "# --- Let's try it out! ---\n",
    "# Use some sentences from our dataset to see if it learned\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"Original PT: 'por que eu'\")\n",
    "print(\"Translation EN:\", translate(\"por que eu\", debug=True))\n",
    "print(\"-\" * 40)\n",
    "print(\"Original PT: 'eu venci'\")\n",
    "print(\"Translation EN:\", translate(\"eu venci\"))\n",
    "print(\"-\" * 20)\n",
    "print(\"Original PT: 'socorro'\")\n",
    "print(\"Translation EN:\", translate(\"socorro\"))\n",
    "print(\"-\" * 20)\n",
    "\n",
    "# Try a new sentence it has never seen (but with words from the vocabulary)\n",
    "print(\"Original PT (New): 'eu te entendo'\")\n",
    "print(\"Translation EN (New):\", translate(\"eu te entendo\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fe8e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# TESTE: Verificar se todas as dependÃªncias estÃ£o disponÃ­veis\n",
    "# ============================================================================\n",
    "\n",
    "print(\"ðŸ§ª TESTE DE DEPENDÃŠNCIAS PARA INFERÃŠNCIA\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# 1. Verificar modelo transformer\n",
    "try:\n",
    "    assert transformer is not None\n",
    "    print(\"âœ… Modelo 'transformer' estÃ¡ disponÃ­vel\")\n",
    "    print(f\"   - Encoder layers: {transformer.encoder.num_layers}\")\n",
    "    print(f\"   - Decoder layers: {transformer.decoder.num_layers}\")\n",
    "except:\n",
    "    print(\"âŒ Modelo 'transformer' NÃƒO estÃ¡ disponÃ­vel\")\n",
    "\n",
    "# 2. Verificar vetorizaÃ§Ã£o\n",
    "try:\n",
    "    assert source_vectorization is not None\n",
    "    assert target_vectorization is not None\n",
    "    print(\"âœ… Camadas de vetorizaÃ§Ã£o disponÃ­veis\")\n",
    "    print(f\"   - Source vocab size: {len(source_vectorization.get_vocabulary())}\")\n",
    "    print(f\"   - Target vocab size: {len(target_vectorization.get_vocabulary())}\")\n",
    "except:\n",
    "    print(\"âŒ Camadas de vetorizaÃ§Ã£o NÃƒO estÃ£o disponÃ­veis\")\n",
    "\n",
    "# 3. Verificar funÃ§Ãµes de mÃ¡scara\n",
    "try:\n",
    "    test_mask = create_padding_mask(tf.constant([[1, 2, 0, 0]]))\n",
    "    print(\"âœ… FunÃ§Ã£o 'create_padding_mask' funciona\")\n",
    "    \n",
    "    test_look_ahead = create_look_ahead_mask(5)\n",
    "    print(\"âœ… FunÃ§Ã£o 'create_look_ahead_mask' funciona\")\n",
    "    \n",
    "    test_la, test_pad = create_decoder_masks(tf.constant([[1, 2]]), tf.constant([[3, 4]]))\n",
    "    print(\"âœ… FunÃ§Ã£o 'create_decoder_masks' funciona\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ FunÃ§Ãµes de mÃ¡scara com erro: {e}\")\n",
    "\n",
    "# 4. Verificar variÃ¡veis de inferÃªncia\n",
    "try:\n",
    "    assert start_token_index is not None\n",
    "    assert end_token_index is not None\n",
    "    assert index_to_word is not None\n",
    "    assert max_output_length is not None\n",
    "    print(\"âœ… VariÃ¡veis de inferÃªncia disponÃ­veis:\")\n",
    "    print(f\"   - start_token_index: {start_token_index}\")\n",
    "    print(f\"   - end_token_index: {end_token_index}\")\n",
    "    print(f\"   - max_output_length: {max_output_length}\")\n",
    "    print(f\"   - index_to_word entries: {len(index_to_word)}\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ VariÃ¡veis de inferÃªncia com erro: {e}\")\n",
    "\n",
    "# 5. Verificar funÃ§Ã£o standardize_text\n",
    "try:\n",
    "    test_clean = standardize_text(\"OlÃ¡, mundo!\")\n",
    "    assert test_clean == \"olÃ¡ mundo\"\n",
    "    print(\"âœ… FunÃ§Ã£o 'standardize_text' funciona\")\n",
    "    print(f\"   Exemplo: 'OlÃ¡, mundo!' â†’ '{test_clean}'\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ FunÃ§Ã£o 'standardize_text' com erro: {e}\")\n",
    "\n",
    "# 6. Teste completo de pipeline de inferÃªncia (sem traduÃ§Ã£o completa)\n",
    "try:\n",
    "    print(\"\\nðŸ”¬ Teste de pipeline de inferÃªncia:\")\n",
    "    \n",
    "    # a) Preprocessar entrada\n",
    "    test_sentence = \"eu amo python\"\n",
    "    cleaned = standardize_text(test_sentence)\n",
    "    print(f\"   1. Preprocessamento: '{test_sentence}' â†’ '{cleaned}'\")\n",
    "    \n",
    "    # b) Vetorizar\n",
    "    input_vec = source_vectorization([cleaned])\n",
    "    print(f\"   2. VetorizaÃ§Ã£o: shape {input_vec.shape}, primeiros tokens: {input_vec.numpy()[0][:5]}\")\n",
    "    \n",
    "    # c) Criar mÃ¡scaras\n",
    "    enc_mask = create_padding_mask(input_vec)\n",
    "    print(f\"   3. MÃ¡scara encoder: shape {enc_mask.shape}\")\n",
    "    \n",
    "    # d) Passar pelo encoder\n",
    "    encoder_out = transformer.encoder(input_vec, training=False, mask=enc_mask)\n",
    "    print(f\"   4. Encoder output: shape {encoder_out.shape}\")\n",
    "    \n",
    "    # e) Preparar decoder input com token [start]\n",
    "    dec_input = tf.constant([[start_token_index]], dtype=tf.int64)\n",
    "    print(f\"   5. Decoder input inicial: {dec_input.numpy()}, palavra: '{index_to_word[start_token_index]}'\")\n",
    "    \n",
    "    # f) Criar mÃ¡scaras do decoder\n",
    "    look_ahead, dec_pad = create_decoder_masks(dec_input, input_vec)\n",
    "    print(f\"   6. MÃ¡scaras decoder: look_ahead {look_ahead.shape}, padding {dec_pad.shape}\")\n",
    "    \n",
    "    # g) Passar pelo decoder\n",
    "    decoder_out = transformer.decoder(dec_input, encoder_out, training=False,\n",
    "                                      look_ahead_mask=look_ahead, padding_mask=dec_pad)\n",
    "    print(f\"   7. Decoder output: shape {decoder_out.shape}\")\n",
    "    \n",
    "    # h) Camada final\n",
    "    predictions = transformer.final_layer(decoder_out)\n",
    "    print(f\"   8. Predictions: shape {predictions.shape}\")\n",
    "    \n",
    "    # i) Pegar token previsto\n",
    "    predicted_id = tf.argmax(predictions[0, -1, :]).numpy()\n",
    "    predicted_word = index_to_word.get(predicted_id, '???')\n",
    "    print(f\"   9. Token previsto: ID={predicted_id}, palavra='{predicted_word}'\")\n",
    "    \n",
    "    print(\"\\nâœ… Pipeline de inferÃªncia completo funciona!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\nâŒ Pipeline de inferÃªncia com erro: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ðŸ“Š RESUMO: Teste de dependÃªncias concluÃ­do\")\n",
    "print(\"=\" * 70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c745e7a",
   "metadata": {},
   "source": [
    "### DiagnÃ³stico do Modelo (VerificaÃ§Ã£o de Colapso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7cbc85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DIAGNÃ“STICO: Vamos verificar se o modelo estÃ¡ realmente treinado\n",
    "# ============================================================================\n",
    "\n",
    "print(\"ðŸ” DIAGNÃ“STICO DO MODELO\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# 1. Verificar se o modelo tem pesos carregados\n",
    "print(\"\\n1. Verificando pesos do modelo:\")\n",
    "encoder_weights = transformer.encoder.get_weights()\n",
    "decoder_weights = transformer.decoder.get_weights()\n",
    "print(f\"   âœ… Encoder tem {len(encoder_weights)} tensores de pesos\")\n",
    "print(f\"   âœ… Decoder tem {len(decoder_weights)} tensores de pesos\")\n",
    "\n",
    "# 2. Verificar distribuiÃ§Ã£o de probabilidades do modelo\n",
    "print(\"\\n2. Verificando distribuiÃ§Ã£o de probabilidades:\")\n",
    "test_input = source_vectorization([\"eu\"])\n",
    "test_decoder = tf.constant([[start_token_index]], dtype=tf.int64)\n",
    "\n",
    "# Processar com o modelo\n",
    "enc_mask = create_padding_mask(test_input)\n",
    "encoder_out = transformer.encoder(test_input, training=False, mask=enc_mask)\n",
    "look_ahead, dec_mask = create_decoder_masks(test_decoder, test_input)\n",
    "decoder_out = transformer.decoder(test_decoder, encoder_out, training=False, \n",
    "                                   look_ahead_mask=look_ahead, padding_mask=dec_mask)\n",
    "predictions = transformer.final_layer(decoder_out)\n",
    "\n",
    "# Verificar as top-5 prediÃ§Ãµes\n",
    "probs = tf.nn.softmax(predictions[0, 0, :])\n",
    "top_5_indices = tf.argsort(probs, direction='DESCENDING')[:5].numpy()\n",
    "top_5_probs = tf.gather(probs, top_5_indices).numpy()\n",
    "\n",
    "print(f\"   Top 5 palavras previstas para 'eu':\")\n",
    "for idx, (token_id, prob) in enumerate(zip(top_5_indices, top_5_probs)):\n",
    "    word = index_to_word.get(token_id, '???')\n",
    "    print(f\"      {idx+1}. '{word}' (ID={token_id}): {prob:.4f}\")\n",
    "\n",
    "# 3. Verificar se o modelo estÃ¡ colapsado (sempre prevÃª o mesmo)\n",
    "print(\"\\n3. Testando variaÃ§Ã£o nas prediÃ§Ãµes:\")\n",
    "test_sentences = [\"eu\", \"vocÃª\", \"ele\", \"nÃ³s\"]\n",
    "predictions_per_word = []\n",
    "\n",
    "for sent in test_sentences:\n",
    "    test_in = source_vectorization([sent])\n",
    "    dec_in = tf.constant([[start_token_index]], dtype=tf.int64)\n",
    "    \n",
    "    enc_mask = create_padding_mask(test_in)\n",
    "    enc_out = transformer.encoder(test_in, training=False, mask=enc_mask)\n",
    "    la_mask, dec_mask = create_decoder_masks(dec_in, test_in)\n",
    "    dec_out = transformer.decoder(dec_in, enc_out, training=False, \n",
    "                                   look_ahead_mask=la_mask, padding_mask=dec_mask)\n",
    "    pred = transformer.final_layer(dec_out)\n",
    "    \n",
    "    predicted_token = tf.argmax(pred[0, 0, :]).numpy()\n",
    "    predicted_word = index_to_word.get(predicted_token, '???')\n",
    "    predictions_per_word.append((sent, predicted_word, predicted_token))\n",
    "    print(f\"   '{sent}' â†’ '{predicted_word}' (ID={predicted_token})\")\n",
    "\n",
    "# 4. DiagnÃ³stico final\n",
    "print(\"\\n4. DiagnÃ³stico:\")\n",
    "unique_predictions = len(set([p[2] for p in predictions_per_word]))\n",
    "if unique_predictions == 1:\n",
    "    print(\"   âš ï¸  PROBLEMA: Modelo colapsado! Sempre prevÃª o mesmo token.\")\n",
    "    print(\"   ðŸ”§ SOLUÃ‡ÃƒO:\")\n",
    "    print(\"      1. Treinar por mais Ã©pocas (pelo menos 20-30)\")\n",
    "    print(\"      2. Verificar se houve overfitting no 'if' durante treinamento\")\n",
    "    print(\"      3. Considerar reduzir o vocabulÃ¡rio (max_tokens=5000)\")\n",
    "    print(\"      4. Aumentar o modelo (d_model=256, dff=512)\")\n",
    "else:\n",
    "    print(f\"   âœ… Modelo gera {unique_predictions} tokens diferentes\")\n",
    "    print(\"   â„¹ï¸  Mas ainda pode precisar de mais treinamento\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06dabd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import tensorflow as tf\n",
    "\n",
    "# Mostra o caminho para o executÃ¡vel do Python que o notebook estÃ¡ usando\n",
    "print(\"Python Executable:\", sys.executable)\n",
    "\n",
    "# Mostra a versÃ£o do TensorFlow que foi importada\n",
    "print(\"TensorFlow Version:\", tf.__version__)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
