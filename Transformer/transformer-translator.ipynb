{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4538d736",
   "metadata": {},
   "source": [
    "# Tradutor Portugu√™s/Ingl√™s utilizando Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e762596",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "97f26487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Mixed Precision ativado: mixed_float16\n",
      "   Compute dtype: float16\n",
      "   Variable dtype: float32\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import string\n",
    "import re\n",
    "\n",
    "os.environ['KERAS_BACKEND'] = 'tensorflow'\n",
    "\n",
    "import keras\n",
    "from keras import layers\n",
    "import tensorflow as tf\n",
    "\n",
    "# ===== OTIMIZA√á√ÉO: Mixed Precision para acelerar treinamento =====\n",
    "# Reduz uso de mem√≥ria e acelera c√°lculos em GPUs modernas\n",
    "from keras import mixed_precision\n",
    "policy = mixed_precision.Policy('mixed_float16')\n",
    "mixed_precision.set_global_policy(policy)\n",
    "print(f\"‚úÖ Mixed Precision ativado: {policy.name}\")\n",
    "print(f\"   Compute dtype: {policy.compute_dtype}\")\n",
    "print(f\"   Variable dtype: {policy.variable_dtype}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53b23c8",
   "metadata": {},
   "source": [
    "## Carregamento dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0a511920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de pares de frases carregados: 168903\n",
      "------------------------------------------------------------\n",
      "Primeiras 3 frases em ingl√™s: ['Go.', 'Go.', 'Hi.']\n",
      "Primeiras 3 frases em portugu√™s: ['Vai.', 'V√°.', 'Oi.']\n",
      "------------------------------------------------------------\n",
      "√öltimas 3 frases em ingl√™s: [\"No matter how much you try to convince people that chocolate is vanilla, it'll still be chocolate, even though you may manage to convince yourself and a few others that it's vanilla.\", 'Some movies make such an impact that one never forgets them. Such is the case with \"Life is Beautiful,\" the emotional Benigni film that mixes drama and comedy in an exceptional manner.', 'A child who is a native speaker usually knows many things about his or her language that a non-native speaker who has been studying for years still does not know and perhaps will never know.']\n",
      "√öltimas 3 frases em portugu√™s: ['N√£o importa o quanto voc√™ tenta convencer os outros de que chocolate √© baunilha, ele ainda ser√° chocolate, mesmo que voc√™ possa convencer a si mesmo e poucos outros de que √© baunilha.', 'Alguns filmes s√£o t√£o marcantes que jamais nos saem da lembran√ßa. √â o caso da emocionante pel√≠cula de Benigni, \"La vita √® bella\", que mistura drama e com√©dia de maneira excepcional.', 'Uma crian√ßa que √© falante nativa geralmente sabe muitas coisas sobre sua l√≠ngua que um falante n√£o-nativo que tem estudado h√° anos ainda n√£o sabe e talvez nunca saber√°.']\n"
     ]
    }
   ],
   "source": [
    "# Vamos usar listas normais do Python, que s√£o perfeitas para isso.\n",
    "portuguese_sentences = []\n",
    "english_sentences = []\n",
    "\n",
    "# O 'with open(...)' garante que o arquivo seja fechado corretamente no final.\n",
    "with open('por.txt', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        # Ignora linhas em branco que possam existir no arquivo\n",
    "        if not line.strip():\n",
    "            continue\n",
    "\n",
    "        # 1. strip() remove espa√ßos/quebras de linha no in√≠cio/fim\n",
    "        # 2. split('\\t') quebra a linha no caractere de tabula√ß√£o\n",
    "        # O novo formato tem 3 colunas: ingl√™s, portugu√™s, metadados\n",
    "        parts = line.strip().split('\\t')\n",
    "        \n",
    "        # Verificar se a linha tem pelo menos 2 colunas (ingl√™s e portugu√™s)\n",
    "        if len(parts) < 2:\n",
    "            continue\n",
    "        \n",
    "        english_part = parts[0]  # Primeira coluna: ingl√™s\n",
    "        portuguese_part = parts[1]  # Segunda coluna: portugu√™s\n",
    "        # parts[2] cont√©m os metadados, que vamos ignorar\n",
    "\n",
    "        english_sentences.append(english_part)\n",
    "        portuguese_sentences.append(portuguese_part)\n",
    "\n",
    "# Vamos conferir quantas frases temos e as 3 primeiras de cada lista\n",
    "print(f\"Total de pares de frases carregados: {len(english_sentences)}\")\n",
    "print(\"-\" * 60)\n",
    "print(\"Primeiras 3 frases em ingl√™s:\", english_sentences[:3])\n",
    "print(\"Primeiras 3 frases em portugu√™s:\", portuguese_sentences[:3])\n",
    "print(\"-\" * 60)\n",
    "print(\"√öltimas 3 frases em ingl√™s:\", english_sentences[-3:])\n",
    "print(\"√öltimas 3 frases em portugu√™s:\", portuguese_sentences[-3:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098d116d",
   "metadata": {},
   "source": [
    "## Pr√©-Processamento: Limpeza, Padroniza√ß√£o e Transforma√ß√£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c57c976b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original English phrase: 'Hi.'\n",
      "Final target phrase:   '[start] hi [end]'\n",
      "--------------------\n",
      "Final source phrases (first 3): ['vai', 'v√°', 'oi']\n",
      "Final target phrases (first 3): ['[start] go [end]', '[start] go [end]', '[start] hi [end]']\n"
     ]
    }
   ],
   "source": [
    "# Let's keep our original lists for comparison\n",
    "# portuguese_sentences = ['V√°.', 'Oi.', 'Corra!', ...]\n",
    "# english_sentences = ['Go.', 'Hi.', 'Run!', ...]\n",
    "\n",
    "def standardize_text(text):\n",
    "    \"\"\"\n",
    "    Cleans and standardizes a single sentence.\n",
    "    1. Converts to lowercase.\n",
    "    2. Removes punctuation.\n",
    "    3. Removes extra whitespace.\n",
    "    \"\"\"\n",
    "    # Lowercase the text\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Create a translation table to remove punctuation\n",
    "    # string.punctuation contains characters like '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    text = text.translate(translator)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# 1. Standardize the source (Portuguese) and target (English) phrases\n",
    "source_phrases_clean = [standardize_text(pt) for pt in portuguese_sentences]\n",
    "target_phrases_clean = [standardize_text(en) for en in english_sentences]\n",
    "\n",
    "# 2. NOW, add the special tokens to the CLEANED target phrases\n",
    "target_phrases_final = [f'[start] {eng} [end]' for eng in target_phrases_clean]\n",
    "\n",
    "# Let's see the transformation for one example\n",
    "print(f\"Original English phrase: '{english_sentences[2]}'\")\n",
    "print(f\"Final target phrase:   '{target_phrases_final[2]}'\")\n",
    "print(\"-\" * 20)\n",
    "print(\"Final source phrases (first 3):\", source_phrases_clean[:3])\n",
    "print(\"Final target phrases (first 3):\", target_phrases_final[:3])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314279ac",
   "metadata": {},
   "source": [
    "## Vetoriza√ß√£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5923efd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üìä AN√ÅLISE DO DATASET:\n",
      "============================================================\n",
      "Portugu√™s - Comprimento m√©dio: 6.0 palavras\n",
      "Portugu√™s - Comprimento m√°ximo: 33 palavras\n",
      "Portugu√™s - Percentil 95: 10 palavras\n",
      "------------------------------------------------------------\n",
      "Ingl√™s - Comprimento m√©dio: 8.0 palavras\n",
      "Ingl√™s - Comprimento m√°ximo: 37 palavras\n",
      "Ingl√™s - Percentil 95: 12 palavras\n",
      "============================================================\n",
      "\n",
      "üéØ CONFIGURA√á√ïES CALCULADAS:\n",
      "============================================================\n",
      "‚úÖ Sequence Length: 14\n",
      "‚úÖ Vocabul√°rio Portugu√™s: 22460 tokens\n",
      "‚úÖ Vocabul√°rio Ingl√™s: 12908 tokens\n",
      "============================================================\n",
      "\n",
      "üìù TESTE DE VETORIZA√á√ÉO:\n",
      "Original Portuguese: 'que'\n",
      "Vectorized: [[3 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
      "------------------------------------------------------------\n",
      "Original English: '[start] who [end]'\n",
      "Vectorized: [[ 2 68  3  0  0  0  0  0  0  0  0  0  0  0]]\n",
      "\n",
      "üîç Primeiras 10 palavras do vocabul√°rio:\n",
      "Portugu√™s: ['', '[UNK]', np.str_('tom'), np.str_('que'), np.str_('o'), np.str_('n√£o'), np.str_('eu'), np.str_('de'), np.str_('a'), np.str_('voc√™')]\n",
      "Ingl√™s: ['', '[UNK]', np.str_('[start]'), np.str_('[end]'), np.str_('tom'), np.str_('i'), np.str_('to'), np.str_('you'), np.str_('the'), np.str_('a')]\n",
      "\n",
      "‚úÖ '[start]' no vocabul√°rio ingl√™s: True\n",
      "‚úÖ '[end]' no vocabul√°rio ingl√™s: True\n"
     ]
    }
   ],
   "source": [
    "# OTIMIZA√á√ÉO: Calcular vocab_size e sequence_length dinamicamente do dataset\n",
    "# Isso garante que o modelo se adapte aos dados reais\n",
    "\n",
    "# Primeiro, vamos analisar os dados para determinar o tamanho m√°ximo de sequ√™ncia\n",
    "import numpy as np\n",
    "\n",
    "# Calcular o comprimento de cada frase (em palavras)\n",
    "source_lengths = [len(phrase.split()) for phrase in source_phrases_clean]\n",
    "target_lengths = [len(phrase.split()) for phrase in target_phrases_final]\n",
    "\n",
    "# Estat√≠sticas dos comprimentos\n",
    "print(\"=\" * 60)\n",
    "print(\"üìä AN√ÅLISE DO DATASET:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Portugu√™s - Comprimento m√©dio: {np.mean(source_lengths):.1f} palavras\")\n",
    "print(f\"Portugu√™s - Comprimento m√°ximo: {np.max(source_lengths)} palavras\")\n",
    "print(f\"Portugu√™s - Percentil 95: {np.percentile(source_lengths, 95):.0f} palavras\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"Ingl√™s - Comprimento m√©dio: {np.mean(target_lengths):.1f} palavras\")\n",
    "print(f\"Ingl√™s - Comprimento m√°ximo: {np.max(target_lengths)} palavras\")\n",
    "print(f\"Ingl√™s - Percentil 95: {np.percentile(target_lengths, 95):.0f} palavras\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Definir sequence_length como o percentil 95 (cobre 95% dos dados)\n",
    "# Adicionar margem de seguran√ßa (+2 para os tokens [start] e [end])\n",
    "sequence_length = int(max(\n",
    "    np.percentile(source_lengths, 95),\n",
    "    np.percentile(target_lengths, 95)\n",
    ")) + 2\n",
    "\n",
    "# Criar as camadas de vetoriza√ß√£o SEM limite de vocab (None = ilimitado)\n",
    "source_vectorization = layers.TextVectorization(\n",
    "    max_tokens=None,  # MUDAN√áA: Sem limite de vocabul√°rio\n",
    "    output_sequence_length=sequence_length,\n",
    "    standardize=None\n",
    ")\n",
    "\n",
    "target_vectorization = layers.TextVectorization(\n",
    "    max_tokens=None,  # MUDAN√áA: Sem limite de vocabul√°rio\n",
    "    output_sequence_length=sequence_length,\n",
    "    standardize=None\n",
    ")\n",
    "\n",
    "# Treinar as camadas nos datasets\n",
    "source_vectorization.adapt(source_phrases_clean)\n",
    "target_vectorization.adapt(target_phrases_final)\n",
    "\n",
    "# CALCULAR vocab_size dinamicamente ap√≥s o adapt()\n",
    "vocab_size_source = len(source_vectorization.get_vocabulary())\n",
    "vocab_size_target = len(target_vectorization.get_vocabulary())\n",
    "\n",
    "print(\"\\nüéØ CONFIGURA√á√ïES CALCULADAS:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"‚úÖ Sequence Length: {sequence_length}\")\n",
    "print(f\"‚úÖ Vocabul√°rio Portugu√™s: {vocab_size_source} tokens\")\n",
    "print(f\"‚úÖ Vocabul√°rio Ingl√™s: {vocab_size_target} tokens\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Testar em um exemplo\n",
    "test_phrase_pt = source_phrases_clean[10]\n",
    "vectorized_pt = source_vectorization([test_phrase_pt])\n",
    "\n",
    "test_phrase_en = target_phrases_final[10]\n",
    "vectorized_en = target_vectorization([test_phrase_en])\n",
    "\n",
    "print(f\"\\nüìù TESTE DE VETORIZA√á√ÉO:\")\n",
    "print(f\"Original Portuguese: '{test_phrase_pt}'\")\n",
    "print(f\"Vectorized: {vectorized_pt.numpy()}\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"Original English: '{test_phrase_en}'\")\n",
    "print(f\"Vectorized: {vectorized_en.numpy()}\")\n",
    "\n",
    "# Verificar tokens especiais\n",
    "pt_vocab = source_vectorization.get_vocabulary()\n",
    "en_vocab = target_vectorization.get_vocabulary()\n",
    "print(f\"\\nüîç Primeiras 10 palavras do vocabul√°rio:\")\n",
    "print(f\"Portugu√™s: {pt_vocab[:10]}\")\n",
    "print(f\"Ingl√™s: {en_vocab[:10]}\")\n",
    "print(f\"\\n‚úÖ '[start]' no vocabul√°rio ingl√™s: {'[start]' in en_vocab}\")\n",
    "print(f\"‚úÖ '[end]' no vocabul√°rio ingl√™s: {'[end]' in en_vocab}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4642b8e6",
   "metadata": {},
   "source": [
    "## Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4cbb96e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(layers.Layer):\n",
    "    \"\"\"\n",
    "    This layer injects positional information into the input embeddings.\n",
    "    It's a fixed, non-learnable layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, max_length, d_model):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model # Dimension of the embedding vector\n",
    "        self.max_length = max_length # Maximum possible length of a sequence\n",
    "\n",
    "        # Create a positional encoding matrix of shape (max_length, d_model)\n",
    "        # This matrix is pre-calculated and will not change during training.\n",
    "        \n",
    "        # Create a tensor representing positions (0, 1, ..., max_length-1)\n",
    "        # Shape: (max_length, 1)\n",
    "        positions = tf.range(start=0, limit=max_length, delta=1, dtype=tf.float32)\n",
    "        positions = tf.expand_dims(positions, axis=1)\n",
    "\n",
    "        # Calculate the denominator term in the formula.\n",
    "        # 2i/d_model --> [0, 2, 4, ..., d_model-2] / d_model\n",
    "        div_term = tf.exp(tf.range(0, d_model, 2, dtype=tf.float32) * -(np.log(10000.0) / d_model))\n",
    "\n",
    "        # Calculate the angles for the sine and cosine functions\n",
    "        # Broadcasting (positions * div_term) results in a shape of (max_length, d_model/2)\n",
    "        angles = positions * div_term\n",
    "\n",
    "        # Calculate sine for even indices and cosine for odd indices\n",
    "        sin_values = tf.sin(angles)\n",
    "        cos_values = tf.cos(angles)\n",
    "        \n",
    "        # Interleave the sine and cosine values.\n",
    "        # For example, if sin=[s1,s2] and cos=[c1,c2], the result is [s1,c1,s2,c2]\n",
    "        # This creates the final positional encoding matrix.\n",
    "        # Shape: (max_length, d_model)\n",
    "        pe = tf.stack([sin_values, cos_values], axis=2)  # Shape: (max_length, d_model/2, 2)\n",
    "        pe = tf.reshape(pe, [max_length, d_model])       # Shape: (max_length, d_model)\n",
    "        \n",
    "        # Store as TensorFlow constant for better integration\n",
    "        self.positional_encoding = tf.constant(pe, dtype=tf.float32)\n",
    "\n",
    "    def call(self, x):\n",
    "        \"\"\"\n",
    "        The forward pass of the layer.\n",
    "        Args:\n",
    "            x: Input embeddings. Shape: (batch_size, sequence_length, d_model)\n",
    "        Returns:\n",
    "            Embeddings with added positional information.\n",
    "        \"\"\"\n",
    "        # Get the length of the input sequence as a Python int\n",
    "        seq_length = x.shape[1] if x.shape[1] is not None else tf.shape(x)[1]\n",
    "        \n",
    "        # Add the positional encoding to the input embeddings.\n",
    "        # We only use the part of the PE matrix that corresponds to the sequence length.\n",
    "        # Use tf.slice for dynamic slicing which works with both static and dynamic shapes\n",
    "        pos_encoding = tf.slice(self.positional_encoding, [0, 0], [seq_length, self.d_model])\n",
    "        \n",
    "        return x + tf.cast(pos_encoding, dtype=x.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "902bd2f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Configura√ß√£o do teste:\n",
      "   d_model: 128\n",
      "   max_length: 14\n",
      "   vocab_size (portugu√™s): 22460\n",
      "   test_seq_len: 10\n",
      "\n",
      "‚úÖ Shape of word embeddings: (2, 10, 128)\n",
      "‚úÖ Shape of final embeddings (with positional info): (2, 10, 128)\n"
     ]
    }
   ],
   "source": [
    "# --- Configuration ---\n",
    "# Usar os valores calculados dinamicamente do dataset\n",
    "d_model = 128  # Dimens√£o dos embeddings\n",
    "max_length = sequence_length  # Usar o sequence_length calculado\n",
    "\n",
    "print(f\"üìä Configura√ß√£o do teste:\")\n",
    "print(f\"   d_model: {d_model}\")\n",
    "print(f\"   max_length: {max_length}\")\n",
    "print(f\"   vocab_size (portugu√™s): {vocab_size_source}\")\n",
    "\n",
    "# --- Create dummy input data ---\n",
    "# CORRE√á√ÉO: O tamanho do dummy_input deve ser <= max_length\n",
    "# Usando max_length-2 para garantir que cabe dentro do limite\n",
    "test_seq_len = min(10, max_length - 1)  # Usa 10 ou max_length-1, o que for menor\n",
    "dummy_input = tf.random.uniform((2, test_seq_len), maxval=vocab_size_source, dtype=tf.int64)\n",
    "\n",
    "print(f\"   test_seq_len: {test_seq_len}\")\n",
    "\n",
    "# --- Build and run the layers ---\n",
    "embedding_layer = layers.Embedding(input_dim=vocab_size_source, output_dim=d_model)\n",
    "positional_encoding_layer = PositionalEncoding(max_length=max_length, d_model=d_model)\n",
    "\n",
    "# 1. Pass input through the embedding layer\n",
    "word_embeddings = embedding_layer(dummy_input)\n",
    "\n",
    "# 2. Add positional information\n",
    "final_embeddings = positional_encoding_layer(word_embeddings)\n",
    "\n",
    "print(f\"\\n‚úÖ Shape of word embeddings: {word_embeddings.shape}\")\n",
    "print(f\"‚úÖ Shape of final embeddings (with positional info): {final_embeddings.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbdbfb00",
   "metadata": {},
   "source": [
    "## Fun√ß√µes de M√°scara (Otimiza√ß√£o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "53cda80c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Testando fun√ß√µes de m√°scara CORRIGIDAS:\n",
      "\n",
      "1. Padding Mask:\n",
      "   Input shape: (2, 4)\n",
      "   Mask shape: (2, 1, 1, 4) ‚Üê DEVE SER (2, 1, 1, 4)\n",
      "   Valores √∫nicos: [-0.e+00 -1.e+09]\n",
      "\n",
      "2. Look-Ahead Mask:\n",
      "   Shape: (5, 5) ‚Üê DEVE SER (5, 5)\n",
      "   Valores:\n",
      "[[-0.e+00 -1.e+09 -1.e+09 -1.e+09 -1.e+09]\n",
      " [-0.e+00 -0.e+00 -1.e+09 -1.e+09 -1.e+09]\n",
      " [-0.e+00 -0.e+00 -0.e+00 -1.e+09 -1.e+09]\n",
      " [-0.e+00 -0.e+00 -0.e+00 -0.e+00 -1.e+09]\n",
      " [-0.e+00 -0.e+00 -0.e+00 -0.e+00 -0.e+00]]\n",
      "\n",
      "3. Decoder Masks Combinadas:\n",
      "   Look-ahead mask shape: (1, 1, 5, 5) ‚Üê DEVE SER (1, 1, 5, 5)\n",
      "   Cross-attention mask shape: (1, 1, 1, 5) ‚Üê DEVE SER (1, 1, 1, 5)\n",
      "\n",
      "‚úÖ M√°scaras corrigidas com sucesso!\n"
     ]
    }
   ],
   "source": [
    "# Substitua a c√©lula de \"Fun√ß√µes de M√°scara\" por esta vers√£o corrigida:\n",
    "\n",
    "def create_padding_mask(seq):\n",
    "    \"\"\"Cria uma m√°scara de adi√ß√£o para zerar a aten√ß√£o nos tokens de padding.\"\"\"\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "    # Adiciona dimens√µes extras para broadcasting com os scores de aten√ß√£o\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :] * -1e9  # Shape: (batch, 1, 1, seq_len)\n",
    "\n",
    "def create_look_ahead_mask(size):\n",
    "    \"\"\"Cria uma m√°scara de adi√ß√£o para zerar a aten√ß√£o em tokens futuros.\"\"\"\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask * -1e9  # Multiplica por um n√∫mero muito negativo\n",
    "\n",
    "def create_decoder_masks(tar, inp):\n",
    "    \"\"\"Cria todas as m√°scaras de adi√ß√£o necess√°rias para o decoder.\"\"\"\n",
    "    # M√°scara look-ahead para a primeira sub-camada de aten√ß√£o do decoder\n",
    "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
    "\n",
    "    # M√°scara de padding para a primeira sub-camada de aten√ß√£o do decoder\n",
    "    dec_target_padding_mask = create_padding_mask(tar)\n",
    "\n",
    "    # Combina as duas m√°scaras para a self-attention do decoder\n",
    "    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
    "\n",
    "    # M√°scara de padding para a segunda sub-camada (cross-attention), que olha para o encoder\n",
    "    cross_attention_mask = create_padding_mask(inp)\n",
    "\n",
    "    return combined_mask, cross_attention_mask\n",
    "\n",
    "\n",
    "# --- Teste das m√°scaras ---\n",
    "print(\"üß™ Testando fun√ß√µes de m√°scara CORRIGIDAS:\")\n",
    "\n",
    "# Teste 1: Padding mask\n",
    "test_seq = tf.constant([[7, 6, 0, 0], [1, 2, 3, 0]])\n",
    "padding_mask = create_padding_mask(test_seq)\n",
    "print(f\"\\n1. Padding Mask:\")\n",
    "print(f\"   Input shape: {test_seq.shape}\")\n",
    "print(f\"   Mask shape: {padding_mask.shape} ‚Üê DEVE SER (2, 1, 1, 4)\")\n",
    "print(f\"   Valores √∫nicos: {tf.unique(tf.reshape(padding_mask, [-1]))[0].numpy()}\")\n",
    "\n",
    "# Teste 2: Look-ahead mask\n",
    "look_mask = create_look_ahead_mask(5)\n",
    "print(f\"\\n2. Look-Ahead Mask:\")\n",
    "print(f\"   Shape: {look_mask.shape} ‚Üê DEVE SER (5, 5)\")\n",
    "print(f\"   Valores:\\n{look_mask.numpy()}\")\n",
    "\n",
    "# Teste 3: M√°scaras combinadas do decoder\n",
    "test_tar = tf.constant([[1, 2, 3, 0, 0]])\n",
    "test_inp = tf.constant([[4, 5, 6, 7, 0]])\n",
    "look_ahead_combined, padding_combined = create_decoder_masks(test_tar, test_inp)\n",
    "print(f\"\\n3. Decoder Masks Combinadas:\")\n",
    "print(f\"   Look-ahead mask shape: {look_ahead_combined.shape} ‚Üê DEVE SER (1, 1, 5, 5)\")\n",
    "print(f\"   Cross-attention mask shape: {padding_combined.shape} ‚Üê DEVE SER (1, 1, 1, 5)\")\n",
    "\n",
    "print(\"\\n‚úÖ M√°scaras corrigidas com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab0efcd",
   "metadata": {},
   "source": [
    "## Self-Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3dd8de95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the input: (2, 15, 128)\n",
      "Shape of the output (from Keras MHA): (2, 15, 128)\n"
     ]
    }
   ],
   "source": [
    "# --- Configuration ---\n",
    "d_model = 128\n",
    "num_heads = 8\n",
    "\n",
    "# The dimension of each head is d_model / num_heads\n",
    "key_dim = d_model // num_heads\n",
    "\n",
    "# --- Create dummy input data ---\n",
    "# Batch of 2 sentences, length 15, embedding dim 128\n",
    "dummy_input = tf.random.uniform((2, 15, d_model))\n",
    "\n",
    "# --- Build and run the layer ---\n",
    "mha_layer = layers.MultiHeadAttention(\n",
    "    num_heads=num_heads,\n",
    "    key_dim=key_dim,\n",
    "    output_shape=d_model # Ensures the output dimension is correct\n",
    ")\n",
    "\n",
    "# In self-attention, query, value, and key are the same.\n",
    "output_keras = mha_layer(query=dummy_input, value=dummy_input, key=dummy_input)\n",
    "\n",
    "print(\"Shape of the input:\", dummy_input.shape)\n",
    "print(\"Shape of the output (from Keras MHA):\", output_keras.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2dd5c44",
   "metadata": {},
   "source": [
    "## Encoder Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e01bea65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Configura√ß√£o do Encoder Block:\n",
      "   d_model: 128\n",
      "   num_heads: 8\n",
      "   dff: 128 (otimizado para velocidade)\n",
      "   dropout_rate: 0.1\n",
      "\n",
      "‚úÖ Shape of the input to the Encoder Block: (2, 15, 128)\n",
      "‚úÖ Shape of the output from the Encoder Block: (2, 15, 128)\n"
     ]
    }
   ],
   "source": [
    "# --- Encoder Block Configuration ---\n",
    "d_model = 128       # Dimension of the model (embedding size)\n",
    "num_heads = 8       # Number of attention heads\n",
    "dff = 128           # OTIMIZA√á√ÉO: Reduzido de 512 para 128 (igual ao exemplo Kaggle)\n",
    "dropout_rate = 0.1  # Dropout rate for regularization\n",
    "\n",
    "print(f\"üìä Configura√ß√£o do Encoder Block:\")\n",
    "print(f\"   d_model: {d_model}\")\n",
    "print(f\"   num_heads: {num_heads}\")\n",
    "print(f\"   dff: {dff} (otimizado para velocidade)\")\n",
    "print(f\"   dropout_rate: {dropout_rate}\")\n",
    "\n",
    "class EncoderBlock(layers.Layer):\n",
    "    \"\"\"\n",
    "    Represents one block of the Transformer's Encoder.\n",
    "    It consists of Multi-Head Attention and a Feed-Forward Network,\n",
    "    with residual connections and layer normalization.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.supports_masking = True\n",
    "\n",
    "        # Multi-Head Attention Layer\n",
    "        self.mha = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim=d_model // num_heads,\n",
    "            output_shape=d_model\n",
    "        )\n",
    "\n",
    "        # Feed-Forward Network (consists of two dense layers)\n",
    "        self.ffn = keras.Sequential([\n",
    "            layers.Dense(dff, activation='relu'), # (batch_size, seq_len, dff)\n",
    "            layers.Dense(d_model)                 # (batch_size, seq_len, d_model)\n",
    "        ])\n",
    "\n",
    "        # Layer Normalization\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        # Dropout for regularization\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, training, padding_mask=None):\n",
    "        # 1. Multi-Head Attention sub-layer\n",
    "        # The input 'x' is used for query, key, and value in self-attention\n",
    "        attn_output = self.mha(query=x, value=x, key=x, attention_mask=padding_mask)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        # Residual connection and Layer Normalization\n",
    "        out1 = self.layernorm1(x + attn_output)\n",
    "\n",
    "        # 2. Feed-Forward Network sub-layer\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        # Residual connection and Layer Normalization\n",
    "        out2 = self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "        return out2\n",
    "\n",
    "# --- Test the EncoderBlock ---\n",
    "encoder_block = EncoderBlock(d_model=d_model, num_heads=num_heads, dff=dff)\n",
    "\n",
    "# Create some dummy input (output from positional encoding)\n",
    "dummy_input = tf.random.uniform((2, 15, d_model)) # (batch_size, sequence_length, d_model)\n",
    "\n",
    "# The call method requires a 'training' flag for dropout\n",
    "output_from_block = encoder_block(dummy_input, training=False)\n",
    "\n",
    "print(f\"\\n‚úÖ Shape of the input to the Encoder Block: {dummy_input.shape}\")\n",
    "print(f\"‚úÖ Shape of the output from the Encoder Block: {output_from_block.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ccd20f",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "65488aea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Configura√ß√£o do Encoder completo:\n",
      "   num_layers: 2\n",
      "   d_model: 128\n",
      "   num_heads: 8\n",
      "   dff: 128\n",
      "   input_vocab_size: 22460\n",
      "   max_sentence_length: 14\n",
      "\n",
      "‚úÖ Shape of the input to the Encoder (vectorized text): (2, 14)\n",
      "‚úÖ Shape of the output from the Encoder (context vectors): (2, 14, 128)\n"
     ]
    }
   ],
   "source": [
    "class Encoder(layers.Layer):\n",
    "    \"\"\"\n",
    "    The complete Encoder, consisting of an embedding layer, positional encoding,\n",
    "    and a stack of N EncoderBlocks.\n",
    "    \n",
    "    OTIMIZA√á√ÉO: Agora usa m√°scaras de padding corretamente!\n",
    "    \"\"\"\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff,\n",
    "                 input_vocab_size, max_length, rate=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.supports_masking = True\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Input Embedding layer\n",
    "        self.embedding = layers.Embedding(input_vocab_size, d_model)\n",
    "        \n",
    "        # Positional Encoding layer\n",
    "        self.pos_encoding = PositionalEncoding(max_length, self.d_model)\n",
    "\n",
    "        # Stack of EncoderBlocks\n",
    "        # We use a list comprehension to create N encoder blocks\n",
    "        self.enc_layers = [EncoderBlock(d_model, num_heads, dff, rate)\n",
    "                           for _ in range(num_layers)]\n",
    "\n",
    "        # Dropout layer for regularization\n",
    "        self.dropout = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, training=False, padding_mask=None):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "\n",
    "        # 1. Get embeddings\n",
    "        x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
    "        \n",
    "        # A scaling factor is applied to the embeddings as in the original paper\n",
    "        # CORRE√á√ÉO: Converter para o mesmo dtype dos embeddings (compat√≠vel com mixed precision)\n",
    "        x *= tf.cast(tf.math.sqrt(tf.cast(self.d_model, tf.float32)), x.dtype)\n",
    "\n",
    "        # 2. Add positional encoding\n",
    "        x = self.pos_encoding(x)\n",
    "\n",
    "        # 3. Apply dropout\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        # 4. Pass through the stack of encoder blocks\n",
    "        # OTIMIZA√á√ÉO: Agora passa a m√°scara para cada bloco\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x, training=training, padding_mask=padding_mask)\n",
    "\n",
    "        return x  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "# --- Test the complete Encoder ---\n",
    "\n",
    "# OTIMIZA√á√ÉO: Usar valores calculados dinamicamente do dataset\n",
    "num_layers_encoder = 2  # Stack 2 blocks for this test\n",
    "d_model = 128\n",
    "num_heads = 8\n",
    "dff = 128  # OTIMIZADO: Reduzido para 128\n",
    "input_vocab_size = vocab_size_source  # CALCULADO DINAMICAMENTE\n",
    "max_sentence_length = sequence_length  # CALCULADO DINAMICAMENTE\n",
    "\n",
    "print(f\"üìä Configura√ß√£o do Encoder completo:\")\n",
    "print(f\"   num_layers: {num_layers_encoder}\")\n",
    "print(f\"   d_model: {d_model}\")\n",
    "print(f\"   num_heads: {num_heads}\")\n",
    "print(f\"   dff: {dff}\")\n",
    "print(f\"   input_vocab_size: {input_vocab_size}\")\n",
    "print(f\"   max_sentence_length: {max_sentence_length}\")\n",
    "\n",
    "# Create an instance of the Encoder\n",
    "encoder = Encoder(num_layers=num_layers_encoder,\n",
    "                  d_model=d_model,\n",
    "                  num_heads=num_heads,\n",
    "                  dff=dff,\n",
    "                  input_vocab_size=input_vocab_size,\n",
    "                  max_length=max_sentence_length)\n",
    "\n",
    "# Create some dummy input (vectorized portuguese sentences)\n",
    "dummy_pt_vector = source_vectorization([\"eu amo pizza\", \"onde fica o banheiro\"])\n",
    "\n",
    "# Get the output from the encoder\n",
    "encoder_output = encoder(dummy_pt_vector, training=False)\n",
    "\n",
    "print(f\"\\n‚úÖ Shape of the input to the Encoder (vectorized text): {dummy_pt_vector.shape}\")\n",
    "print(f\"‚úÖ Shape of the output from the Encoder (context vectors): {encoder_output.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e385314",
   "metadata": {},
   "source": [
    "## Decoder Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d2d25187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Configura√ß√£o do Decoder Block:\n",
      "   d_model: 128\n",
      "   num_heads: 8\n",
      "   dff: 128 (otimizado)\n",
      "\n",
      "‚úÖ Shape of the decoder input: (2, 15, 128)\n",
      "‚úÖ Shape of the encoder output: (2, 20, 128)\n",
      "‚úÖ Shape of the output from the Decoder Block: (2, 15, 128)\n"
     ]
    }
   ],
   "source": [
    "# This code builds on the previous cells in your notebook.\n",
    "# Make sure you have run the cells defining d_model, num_heads, dff etc.\n",
    "\n",
    "class DecoderBlock(layers.Layer):\n",
    "    \"\"\"\n",
    "    Represents one block of the Transformer's Decoder.\n",
    "    It consists of Masked Multi-Head Self-Attention, Cross-Attention,\n",
    "    and a Feed-Forward Network.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.supports_masking = True\n",
    "\n",
    "        # 1. Masked Multi-Head Self-Attention (looks at the target sequence)\n",
    "        self.mha1 = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim=d_model // num_heads,\n",
    "            output_shape=d_model\n",
    "        )\n",
    "\n",
    "        # 2. Multi-Head Cross-Attention (looks at the encoder output)\n",
    "        self.mha2 = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim=d_model // num_heads,\n",
    "            output_shape=d_model\n",
    "        )\n",
    "\n",
    "        # 3. Feed-Forward Network\n",
    "        self.ffn = keras.Sequential([\n",
    "            layers.Dense(dff, activation='relu'),\n",
    "            layers.Dense(d_model)\n",
    "        ])\n",
    "\n",
    "        # Layer Normalization layers\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        # Dropout layers\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "        self.dropout3 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, encoder_output, training, look_ahead_mask, cross_attention_mask):\n",
    "        # Sub-layer 1: Masked Multi-Head Self-Attention\n",
    "        # The decoder attends to itself, but can't see future tokens.\n",
    "        attn1 = self.mha1(query=x, value=x, key=x, attention_mask=look_ahead_mask)\n",
    "        attn1 = self.dropout1(attn1, training=training)\n",
    "        out1 = self.layernorm1(x + attn1)\n",
    "\n",
    "        # Sub-layer 2: Cross-Attention\n",
    "        # The decoder attends to the encoder's output.\n",
    "        # Query comes from the decoder, Key and Value from the encoder.\n",
    "        # Usa a m√°scara de padding vinda do encoder.\n",
    "        attn2 = self.mha2(query=out1, value=encoder_output, key=encoder_output, attention_mask=cross_attention_mask)\n",
    "        attn2 = self.dropout2(attn2, training=training)\n",
    "        out2 = self.layernorm2(out1 + attn2)\n",
    "\n",
    "        # Sub-layer 3: Feed-Forward Network\n",
    "        ffn_output = self.ffn(out2)\n",
    "        ffn_output = self.dropout3(ffn_output, training=training)\n",
    "        out3 = self.layernorm3(out2 + ffn_output)\n",
    "\n",
    "        return out3\n",
    "\n",
    "# --- Test the DecoderBlock ---\n",
    "# OTIMIZA√á√ÉO: Usar valores otimizados\n",
    "d_model = 128\n",
    "num_heads = 8\n",
    "dff = 128  # OTIMIZADO: Reduzido para 128\n",
    "\n",
    "print(f\"üìä Configura√ß√£o do Decoder Block:\")\n",
    "print(f\"   d_model: {d_model}\")\n",
    "print(f\"   num_heads: {num_heads}\")\n",
    "print(f\"   dff: {dff} (otimizado)\")\n",
    "\n",
    "decoder_block = DecoderBlock(d_model=d_model, num_heads=num_heads, dff=dff)\n",
    "\n",
    "# Create dummy inputs\n",
    "# Note: The sequence lengths for the decoder input and encoder output can be different.\n",
    "dummy_decoder_input = tf.random.uniform((2, 15, d_model)) # (batch_size, target_seq_len, d_model)\n",
    "dummy_encoder_output = tf.random.uniform((2, 20, d_model)) # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "# For testing shapes, we can pass masks as None\n",
    "output_from_block = decoder_block(\n",
    "    dummy_decoder_input,\n",
    "    dummy_encoder_output,\n",
    "    training=False,\n",
    "    look_ahead_mask=None,\n",
    "    cross_attention_mask=None\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Shape of the decoder input: {dummy_decoder_input.shape}\")\n",
    "print(f\"‚úÖ Shape of the encoder output: {dummy_encoder_output.shape}\")\n",
    "print(f\"‚úÖ Shape of the output from the Decoder Block: {output_from_block.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11de9328",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2928128c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Configura√ß√£o do Decoder completo:\n",
      "   num_layers: 2\n",
      "   d_model: 128\n",
      "   num_heads: 8\n",
      "   dff: 128\n",
      "   target_vocab_size: 12908\n",
      "   max_sentence_length: 14\n",
      "\n",
      "‚úÖ Shape of the decoder input (vectorized English text): (2, 14)\n",
      "‚úÖ Shape of the encoder output (context vectors): (2, 14, 128)\n",
      "‚úÖ Shape of the final decoder output: (2, 14, 128)\n"
     ]
    }
   ],
   "source": [
    "class Decoder(layers.Layer):\n",
    "    \"\"\"\n",
    "    The complete Decoder, consisting of an embedding layer, positional encoding,\n",
    "    and a stack of N DecoderBlocks.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff,\n",
    "                 target_vocab_size, max_length, rate=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.supports_masking = True\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Input Embedding layer for the target language\n",
    "        self.embedding = layers.Embedding(target_vocab_size, d_model)\n",
    "\n",
    "        # Positional Encoding layer\n",
    "        self.pos_encoding = PositionalEncoding(max_length, d_model)\n",
    "\n",
    "        # Stack of DecoderBlocks\n",
    "        self.dec_layers = [DecoderBlock(d_model, num_heads, dff, rate)\n",
    "                           for _ in range(num_layers)]\n",
    "\n",
    "        # Dropout layer\n",
    "        self.dropout = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, encoder_output, training=False, look_ahead_mask=None, cross_attention_mask=None):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "\n",
    "        # 1. Get embeddings for the target sequence\n",
    "        x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
    "        # CORRE√á√ÉO: Converter para o mesmo dtype dos embeddings (compat√≠vel com mixed precision)\n",
    "        x *= tf.cast(tf.math.sqrt(tf.cast(self.d_model, tf.float32)), x.dtype)\n",
    "\n",
    "        # 2. Add positional encoding\n",
    "        x = self.pos_encoding(x)\n",
    "\n",
    "        # 3. Apply dropout\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        # 4. Pass through the stack of decoder blocks\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.dec_layers[i](x, encoder_output, training=training,\n",
    "                                     look_ahead_mask=look_ahead_mask,\n",
    "                                     cross_attention_mask=cross_attention_mask)\n",
    "\n",
    "        # The shape of x is (batch_size, target_seq_len, d_model)\n",
    "        return x\n",
    "\n",
    "# --- Test the complete Decoder ---\n",
    "\n",
    "# OTIMIZA√á√ÉO: Usar valores calculados dinamicamente\n",
    "num_layers_decoder = 2  # Stack 2 blocks\n",
    "d_model = 128\n",
    "num_heads = 8\n",
    "dff = 128  # OTIMIZADO: Reduzido para 128\n",
    "target_vocab_size = vocab_size_target  # CALCULADO DINAMICAMENTE\n",
    "max_sentence_length = sequence_length  # CALCULADO DINAMICAMENTE\n",
    "\n",
    "print(f\"üìä Configura√ß√£o do Decoder completo:\")\n",
    "print(f\"   num_layers: {num_layers_decoder}\")\n",
    "print(f\"   d_model: {d_model}\")\n",
    "print(f\"   num_heads: {num_heads}\")\n",
    "print(f\"   dff: {dff}\")\n",
    "print(f\"   target_vocab_size: {target_vocab_size}\")\n",
    "print(f\"   max_sentence_length: {max_sentence_length}\")\n",
    "\n",
    "# Create an instance of the Decoder\n",
    "decoder = Decoder(num_layers=num_layers_decoder,\n",
    "                  d_model=d_model,\n",
    "                  num_heads=num_heads,\n",
    "                  dff=dff,\n",
    "                  target_vocab_size=target_vocab_size,\n",
    "                  max_length=max_sentence_length)\n",
    "\n",
    "# Create dummy inputs\n",
    "# We need both a dummy decoder input and the dummy encoder output from the previous test\n",
    "dummy_en_vector = target_vectorization([\"[start] i love pizza\", \"[start] where is the bathroom\"])\n",
    "# We can reuse the encoder_output from the Encoder test cell\n",
    "# encoder_output has shape (2, sequence_length, 128)\n",
    "\n",
    "# Get the output from the decoder\n",
    "decoder_output = decoder(\n",
    "    x=dummy_en_vector,\n",
    "    encoder_output=encoder_output, # This comes from the Encoder\n",
    "    training=False,\n",
    "    look_ahead_mask=None # Passing None for shape testing\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Shape of the decoder input (vectorized English text): {dummy_en_vector.shape}\")\n",
    "print(f\"‚úÖ Shape of the encoder output (context vectors): {encoder_output.shape}\")\n",
    "print(f\"‚úÖ Shape of the final decoder output: {decoder_output.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c86c200",
   "metadata": {},
   "source": [
    "## Modelo Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "039ef33c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "üöÄ CRIANDO MODELO TRANSFORMER COM CONFIGURA√á√ïES OTIMIZADAS\n",
      "======================================================================\n",
      "   num_layers          : 2\n",
      "   d_model             : 128\n",
      "   num_heads           : 8\n",
      "   dff                 : 128\n",
      "   input_vocab_size    : 22460\n",
      "   target_vocab_size   : 12908\n",
      "   max_length          : 200\n",
      "======================================================================\n",
      "\n",
      "‚úÖ Shape of Portuguese input: (2, 14)\n",
      "‚úÖ Shape of English input: (2, 14)\n",
      "‚úÖ Shape of the final Transformer output (logits): (2, 14, 12908)\n",
      "\n",
      "üéâ Modelo Transformer criado com sucesso!\n"
     ]
    }
   ],
   "source": [
    "class Transformer(keras.Model):\n",
    "    \"\"\"\n",
    "    The complete Transformer model, connecting the Encoder and Decoder.\n",
    "    \n",
    "    OTIMIZA√á√ÉO: Agora cria e usa m√°scaras de padding corretamente!\n",
    "    Isso melhora a acur√°cia e acelera a converg√™ncia.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff,\n",
    "                 input_vocab_size, target_vocab_size, max_length, rate=0.1):\n",
    "        super().__init__()\n",
    "        # Instantiate the Encoder\n",
    "        self.encoder = Encoder(num_layers, d_model, num_heads, dff,\n",
    "                               input_vocab_size, max_length, rate)\n",
    "\n",
    "        # Instantiate the Decoder\n",
    "        self.decoder = Decoder(num_layers, d_model, num_heads, dff,\n",
    "                               target_vocab_size, max_length, rate)\n",
    "\n",
    "        # The final linear layer to project decoder output to vocabulary size\n",
    "        # OTIMIZA√á√ÉO: dtype='float32' para compatibilidade com mixed precision\n",
    "        self.final_layer = layers.Dense(target_vocab_size, dtype='float32')\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        # The 'inputs' will be a tuple: (source_sequence, target_sequence)\n",
    "        inp, tar = inputs\n",
    "\n",
    "        # Criar m√°scaras de padding\n",
    "        enc_padding_mask = create_padding_mask(inp)\n",
    "        \n",
    "        # Criar as duas m√°scaras para o decoder\n",
    "        look_ahead_mask, cross_attention_mask = create_decoder_masks(tar, inp)\n",
    "\n",
    "        # 1. Pass the source sequence through the encoder\n",
    "        encoder_output = self.encoder(inp, training=training, padding_mask=enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n",
    "\n",
    "        # 2. Pass the encoder output and the target sequence through the decoder\n",
    "        decoder_output = self.decoder(\n",
    "            tar, encoder_output, training=training, \n",
    "            look_ahead_mask=look_ahead_mask,\n",
    "            cross_attention_mask=cross_attention_mask\n",
    "        ) # (batch_size, tar_seq_len, d_model)\n",
    "\n",
    "        # 3. Pass the decoder output through the final linear layer\n",
    "        final_output = self.final_layer(decoder_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
    "\n",
    "        return final_output\n",
    "\n",
    "# --- Test the complete Transformer model ---\n",
    "\n",
    "# OTIMIZA√á√ÉO: Usar configura√ß√µes calculadas dinamicamente do dataset\n",
    "print(\"=\" * 70)\n",
    "print(\"üöÄ CRIANDO MODELO TRANSFORMER COM CONFIGURA√á√ïES OTIMIZADAS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "PE_MAX_LENGTH = 200\n",
    "\n",
    "config_summary = {\n",
    "    \"num_layers\": num_layers_encoder,\n",
    "    \"d_model\": d_model,\n",
    "    \"num_heads\": num_heads,\n",
    "    \"dff\": dff,\n",
    "    \"input_vocab_size\": vocab_size_source,\n",
    "    \"target_vocab_size\": vocab_size_target,\n",
    "    \"max_length\": PE_MAX_LENGTH\n",
    "}\n",
    "\n",
    "for key, value in config_summary.items():\n",
    "    print(f\"   {key:20s}: {value}\")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "\n",
    "transformer = Transformer(\n",
    "    num_layers=num_layers_encoder,\n",
    "    d_model=d_model,\n",
    "    num_heads=num_heads,\n",
    "    dff=dff,\n",
    "    input_vocab_size=vocab_size_source,\n",
    "    target_vocab_size=vocab_size_target,\n",
    "    max_length=PE_MAX_LENGTH\n",
    ")\n",
    "\n",
    "# Reuse the dummy vectors from previous tests\n",
    "# The call method expects a tuple of (input, target)\n",
    "output = transformer((dummy_pt_vector, dummy_en_vector), training=False)\n",
    "\n",
    "print(f\"\\n‚úÖ Shape of Portuguese input: {dummy_pt_vector.shape}\")\n",
    "print(f\"‚úÖ Shape of English input: {dummy_en_vector.shape}\")\n",
    "print(f\"‚úÖ Shape of the final Transformer output (logits): {output.shape}\")\n",
    "print(f\"\\nüéâ Modelo Transformer criado com sucesso!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151ab3b8",
   "metadata": {},
   "source": [
    "## Fun√ß√£o de Perda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "cc4a24d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to define our loss function and metrics\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none' # 'none' means we get a loss for each example\n",
    ")\n",
    "\n",
    "def masked_loss_function(real, pred):\n",
    "    # Create a mask to identify where the real target is not a padding token (0)\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    \n",
    "    # Calculate the loss for every token in the batch\n",
    "    loss_ = loss_object(real, pred)\n",
    "    \n",
    "    # Cast the mask to the same type as the loss\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    \n",
    "    # Apply the mask to the loss, effectively zeroing out the loss for padding tokens\n",
    "    loss_ *= mask\n",
    "    \n",
    "    # Return the average loss over the non-padded tokens\n",
    "    return tf.reduce_sum(loss_) / tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a55696",
   "metadata": {},
   "source": [
    "## Learning Rate Schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0bfa97f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super().__init__()\n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        step = tf.cast(step, tf.float32)\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "    \n",
    "    def get_config(self):\n",
    "        \"\"\"Retorna a configura√ß√£o para serializa√ß√£o.\"\"\"\n",
    "        return {\n",
    "            \"d_model\": int(self.d_model.numpy()),\n",
    "            \"warmup_steps\": self.warmup_steps\n",
    "        }\n",
    "\n",
    "# Instantiate the learning rate schedule and the Adam optimizer\n",
    "learning_rate = CustomSchedule(d_model)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516f056c",
   "metadata": {},
   "source": [
    "## Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "badbbdc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Prepare the Data for Training and Validation ---\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Ensure eager execution is enabled\n",
    "#tf.config.run_functions_eagerly(True)\n",
    "\n",
    "# OTIMIZA√á√ÉO: Aumentar batch size para acelerar treinamento\n",
    "# Batch maior = menos steps por √©poca = treinamento mais r√°pido\n",
    "# Valor anterior: 64 | Novo valor: 256 (4x mais r√°pido)\n",
    "batch_size = 512  # Ajuste para 128 se tiver problemas de mem√≥ria\n",
    "\n",
    "# Vectorize all the sentences\n",
    "source_vectors = source_vectorization(source_phrases_clean)\n",
    "target_vectors = target_vectorization(target_phrases_final)\n",
    "\n",
    "# Convert to numpy arrays for sklearn compatibility\n",
    "source_vectors_np = source_vectors.numpy()\n",
    "target_vectors_np = target_vectors.numpy()\n",
    "\n",
    "# Split the data into training and validation sets (80/20 split)\n",
    "source_train, source_val, target_train, target_val = train_test_split(\n",
    "    source_vectors_np, target_vectors_np, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Total examples: {len(source_vectors_np)}\")\n",
    "print(f\"Training examples: {len(source_train)}\")\n",
    "print(f\"Validation examples: {len(source_val)}\")\n",
    "\n",
    "# Helper function to create datasets\n",
    "def create_dataset(source, target):\n",
    "    \"\"\"Helper function to create a tf.data.Dataset with shifted targets.\"\"\"\n",
    "    # The input to the decoder is the target sequence without the last token\n",
    "    decoder_inputs = target[:, :-1]\n",
    "    # The target for the loss function is the target sequence without the first token\n",
    "    decoder_outputs = target[:, 1:]\n",
    "    \n",
    "    # Create the tf.data.Dataset object\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(\n",
    "        ((source, decoder_inputs), decoder_outputs)\n",
    "    )\n",
    "    # Shuffle, batch, and prefetch\n",
    "    return dataset.shuffle(buffer_size=len(source)).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Create the training and validation datasets\n",
    "train_dataset = create_dataset(source_train, target_train)\n",
    "val_dataset = create_dataset(source_val, target_val)\n",
    "\n",
    "# --- 2. Compile the Model ---\n",
    "\n",
    "# We need a metric to monitor performance, let's create a masked accuracy\n",
    "def masked_accuracy(real, pred):\n",
    "    # Get the predicted word by finding the index with the highest logit\n",
    "    pred = tf.argmax(pred, axis=2)\n",
    "    real = tf.cast(real, pred.dtype)\n",
    "    \n",
    "    # Check for matches\n",
    "    match = tf.cast(real == pred, dtype=tf.float32)\n",
    "    \n",
    "    # Create the padding mask\n",
    "    mask = tf.cast(tf.math.logical_not(tf.math.equal(real, 0)), dtype=tf.float32)\n",
    "    \n",
    "    # Apply the mask to the matches\n",
    "    match *= mask\n",
    "    \n",
    "    # Calculate the accuracy on non-padded tokens\n",
    "    return tf.reduce_sum(match) / tf.reduce_sum(mask)\n",
    "\n",
    "# Compile the model with the optimizer, loss, and metric\n",
    "transformer.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=masked_loss_function,\n",
    "    metrics=[masked_accuracy]\n",
    "    #run_eagerly=True  # Run in eager mode to avoid graph mode issues\n",
    ")\n",
    "\n",
    "# --- 3. Configure Early Stopping ---\n",
    "\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Early Stopping: para o treinamento se val_loss n√£o melhorar por 'patience' √©pocas\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',           # Monitora a perda de valida√ß√£o\n",
    "    patience=2,                   # Espera 2 √©pocas sem melhora antes de parar\n",
    "    restore_best_weights=True,    # Restaura os pesos da melhor √©poca\n",
    "    verbose=1,                    # Mostra mensagens quando parar\n",
    "    mode='min'                    # 'min' porque queremos minimizar a loss\n",
    ")\n",
    "\n",
    "# OTIMIZA√á√ÉO: Model Checkpoint - salva o melhor modelo automaticamente\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    filepath='best_transformer_model.keras',  # Nome do arquivo\n",
    "    monitor='val_loss',                       # M√©trica a monitorar\n",
    "    save_best_only=True,                      # Salva apenas quando melhorar\n",
    "    save_weights_only=False,                  # Salva modelo completo\n",
    "    mode='min',                               # Minimizar a loss\n",
    "    verbose=1                                 # Mostrar quando salvar\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Callbacks configurados:\")\n",
    "print(f\"   - Early Stopping (patience={early_stopping.patience})\")\n",
    "print(\"   - Model Checkpoint (salva em 'best_transformer_model.keras')\")\n",
    "\n",
    "# --- 4. Train the Model with Validation and Early Stopping ---\n",
    "\n",
    "print(\"Starting training with validation and Early Stopping...\")\n",
    "print(f\"Training will stop automatically if validation loss doesn't improve for {early_stopping.patience} epochs.\")\n",
    "print(\"=\" * 80)\n",
    "print(\"üöÄ OTIMIZA√á√ïES ATIVAS:\")\n",
    "print(\"   ‚úÖ Mixed Precision (float16) - Acelera c√°lculos\")\n",
    "print(\"   ‚úÖ M√°scaras de Padding - Melhora acur√°cia e converg√™ncia\")\n",
    "print(f\"   ‚úÖ Batch Size: {batch_size} - Acelera treinamento\")\n",
    "print(\"   ‚úÖ Model Checkpoint - Salva melhor modelo automaticamente\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Definimos mais √©pocas, mas o early stopping pode parar antes\n",
    "epochs = 5 # 100\n",
    "\n",
    "history = transformer.fit(\n",
    "    train_dataset,\n",
    "    epochs=epochs,\n",
    "    validation_data=val_dataset,\n",
    "    callbacks=[early_stopping, model_checkpoint]  # OTIMIZA√á√ÉO: Adiciona checkpoint\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Training finished!\")\n",
    "print(f\"Total epochs trained: {len(history.history['loss'])}\")\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e693fc5",
   "metadata": {},
   "source": [
    "## Carregando o Melhor Modelo (Opcional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ac91bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se voc√™ quiser carregar o melhor modelo salvo pelo checkpoint:\n",
    "# (√ötil se voc√™ reiniciar o kernel ou quiser usar o modelo em outra sess√£o)\n",
    "\n",
    "# Para carregar o modelo completo:\n",
    "# transformer = keras.models.load_model('best_transformer_model.keras')\n",
    "\n",
    "# OU para apenas verificar se o arquivo existe:\n",
    "# import os\n",
    "# if os.path.exists('best_transformer_model.keras'):\n",
    "#     print(\"‚úÖ Melhor modelo salvo em: best_transformer_model.keras\")\n",
    "#     print(f\"   Tamanho do arquivo: {os.path.getsize('best_transformer_model.keras') / 1024 / 1024:.2f} MB\")\n",
    "# else:\n",
    "#     print(\"‚ö†Ô∏è Modelo ainda n√£o foi salvo (treine primeiro)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7373d1fa",
   "metadata": {},
   "source": [
    "## Visualiza√ß√£o das Curvas de Aprendizagem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a0a222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the training history\n",
    "train_acc = history.history['masked_accuracy']\n",
    "val_acc = history.history['val_masked_accuracy']\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "# Create the epochs range\n",
    "epochs_range = range(len(train_acc))\n",
    "\n",
    "# Plot the training and validation curves\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# --- Plot Accuracy ---\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, train_acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True)\n",
    "\n",
    "# --- Plot Loss ---\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, train_loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nFinal Training Loss: {train_loss[-1]:.4f}\")\n",
    "print(f\"Final Validation Loss: {val_loss[-1]:.4f}\")\n",
    "print(f\"Final Training Accuracy: {train_acc[-1]:.4f}\")\n",
    "print(f\"Final Validation Accuracy: {val_acc[-1]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0005e661",
   "metadata": {},
   "source": [
    "## Infer√™ncia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4fe42c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the target vocabulary\n",
    "target_vocab = target_vectorization.get_vocabulary()\n",
    "\n",
    "# Create a dictionary to map the target language index back to a word\n",
    "index_to_word = {i: word for i, word in enumerate(target_vocab)}\n",
    "\n",
    "# Define the maximum length for a generated translation\n",
    "max_output_length = 20\n",
    "\n",
    "# Find the indices for start and end tokens (with brackets preserved)\n",
    "start_token_index = target_vocab.index('[start]')\n",
    "end_token_index = target_vocab.index('[end]')\n",
    "\n",
    "print(f\"Token '[start]' tem √≠ndice: {start_token_index}\")\n",
    "print(f\"Token '[end]' tem √≠ndice: {end_token_index}\")\n",
    "\n",
    "def translate(sentence, debug=False):\n",
    "    \"\"\"\n",
    "    Translates a Portuguese sentence to English using the trained Transformer model.\n",
    "    \"\"\"\n",
    "    # 1. Preprocess the input sentence\n",
    "    cleaned_sentence = standardize_text(sentence)\n",
    "    \n",
    "    # 2. Vectorize the sentence and add a batch dimension\n",
    "    input_vector = source_vectorization([cleaned_sentence]) # Shape: (1, sequence_length)\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"  Input vectorizado: {input_vector.numpy()}\")\n",
    "    \n",
    "    # 3. Initialize the decoder's input with the '[start]' token\n",
    "    # The decoder input starts as a tensor with the start token index\n",
    "    decoder_input = tf.constant([[start_token_index]], dtype=tf.int64)\n",
    "\n",
    "    generated_tokens = []\n",
    "    for i in range(max_output_length):\n",
    "        # 4. Get the model's predictions\n",
    "        # The model is called in a non-training mode\n",
    "        predictions = transformer((input_vector, decoder_input), training=False)\n",
    "        \n",
    "        # 5. Get the logits for the very last predicted token\n",
    "        # predictions shape: (batch_size, seq_len, vocab_size) -> (1, i+1, vocab_size)\n",
    "        # We want the predictions for the last token in the sequence\n",
    "        last_token_logits = predictions[:, -1, :] # Shape: (1, vocab_size)\n",
    "        \n",
    "        # 6. Find the token with the highest probability (greedy search)\n",
    "        predicted_id = tf.argmax(last_token_logits, axis=-1) # Shape: (1,)\n",
    "        predicted_id_value = int(predicted_id.numpy()[0])\n",
    "        \n",
    "        if debug:\n",
    "            print(f\"  Step {i}: Predicted token ID = {predicted_id_value}, word = '{index_to_word.get(predicted_id_value, '???')}'\")\n",
    "        \n",
    "        generated_tokens.append(predicted_id_value)\n",
    "        \n",
    "        # 7. Add the predicted token to the decoder input for the next iteration\n",
    "        decoder_input = tf.concat([decoder_input, [predicted_id]], axis=1)\n",
    "        \n",
    "        # 8. Check if the predicted token is the '[end]' token\n",
    "        if predicted_id_value == end_token_index:\n",
    "            if debug:\n",
    "                print(f\"  Token '[end]' found, stopping generation.\")\n",
    "            break\n",
    "            \n",
    "    # 9. Convert the sequence of token IDs back to words\n",
    "    # We ignore the [start] and [end] tokens\n",
    "    output_tokens = [token for token in generated_tokens if token != end_token_index]\n",
    "    translated_text = \" \".join(index_to_word[token] for token in output_tokens)\n",
    "    \n",
    "    return translated_text\n",
    "\n",
    "# --- Let's try it out! ---\n",
    "# Use some sentences from our dataset to see if it learned\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"Original PT: 'por que eu'\")\n",
    "print(\"Translation EN:\", translate(\"por que eu\", debug=True))\n",
    "print(\"-\" * 40)\n",
    "print(\"Original PT: 'eu venci'\")\n",
    "print(\"Translation EN:\", translate(\"eu venci\"))\n",
    "print(\"-\" * 20)\n",
    "print(\"Original PT: 'socorro'\")\n",
    "print(\"Translation EN:\", translate(\"socorro\"))\n",
    "print(\"-\" * 20)\n",
    "\n",
    "# Try a new sentence it has never seen (but with words from the vocabulary)\n",
    "print(\"Original PT (New): 'eu te entendo'\")\n",
    "print(\"Translation EN (New):\", translate(\"eu te entendo\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fe8e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# TESTE: Verificar se todas as depend√™ncias est√£o dispon√≠veis\n",
    "# ============================================================================\n",
    "\n",
    "print(\"üß™ TESTE DE DEPEND√äNCIAS PARA INFER√äNCIA\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# 1. Verificar modelo transformer\n",
    "try:\n",
    "    assert transformer is not None\n",
    "    print(\"‚úÖ Modelo 'transformer' est√° dispon√≠vel\")\n",
    "    print(f\"   - Encoder layers: {transformer.encoder.num_layers}\")\n",
    "    print(f\"   - Decoder layers: {transformer.decoder.num_layers}\")\n",
    "except:\n",
    "    print(\"‚ùå Modelo 'transformer' N√ÉO est√° dispon√≠vel\")\n",
    "\n",
    "# 2. Verificar vetoriza√ß√£o\n",
    "try:\n",
    "    assert source_vectorization is not None\n",
    "    assert target_vectorization is not None\n",
    "    print(\"‚úÖ Camadas de vetoriza√ß√£o dispon√≠veis\")\n",
    "    print(f\"   - Source vocab size: {len(source_vectorization.get_vocabulary())}\")\n",
    "    print(f\"   - Target vocab size: {len(target_vectorization.get_vocabulary())}\")\n",
    "except:\n",
    "    print(\"‚ùå Camadas de vetoriza√ß√£o N√ÉO est√£o dispon√≠veis\")\n",
    "\n",
    "# 3. Verificar fun√ß√µes de m√°scara\n",
    "try:\n",
    "    test_mask = create_padding_mask(tf.constant([[1, 2, 0, 0]]))\n",
    "    print(\"‚úÖ Fun√ß√£o 'create_padding_mask' funciona\")\n",
    "    \n",
    "    test_look_ahead = create_look_ahead_mask(5)\n",
    "    print(\"‚úÖ Fun√ß√£o 'create_look_ahead_mask' funciona\")\n",
    "    \n",
    "    test_la, test_pad = create_decoder_masks(tf.constant([[1, 2]]), tf.constant([[3, 4]]))\n",
    "    print(\"‚úÖ Fun√ß√£o 'create_decoder_masks' funciona\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Fun√ß√µes de m√°scara com erro: {e}\")\n",
    "\n",
    "# 4. Verificar vari√°veis de infer√™ncia\n",
    "try:\n",
    "    assert start_token_index is not None\n",
    "    assert end_token_index is not None\n",
    "    assert index_to_word is not None\n",
    "    assert max_output_length is not None\n",
    "    print(\"‚úÖ Vari√°veis de infer√™ncia dispon√≠veis:\")\n",
    "    print(f\"   - start_token_index: {start_token_index}\")\n",
    "    print(f\"   - end_token_index: {end_token_index}\")\n",
    "    print(f\"   - max_output_length: {max_output_length}\")\n",
    "    print(f\"   - index_to_word entries: {len(index_to_word)}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Vari√°veis de infer√™ncia com erro: {e}\")\n",
    "\n",
    "# 5. Verificar fun√ß√£o standardize_text\n",
    "try:\n",
    "    test_clean = standardize_text(\"Ol√°, mundo!\")\n",
    "    assert test_clean == \"ol√° mundo\"\n",
    "    print(\"‚úÖ Fun√ß√£o 'standardize_text' funciona\")\n",
    "    print(f\"   Exemplo: 'Ol√°, mundo!' ‚Üí '{test_clean}'\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Fun√ß√£o 'standardize_text' com erro: {e}\")\n",
    "\n",
    "# 6. Teste completo de pipeline de infer√™ncia (sem tradu√ß√£o completa)\n",
    "try:\n",
    "    print(\"\\nüî¨ Teste de pipeline de infer√™ncia:\")\n",
    "    \n",
    "    # a) Preprocessar entrada\n",
    "    test_sentence = \"eu amo python\"\n",
    "    cleaned = standardize_text(test_sentence)\n",
    "    print(f\"   1. Preprocessamento: '{test_sentence}' ‚Üí '{cleaned}'\")\n",
    "    \n",
    "    # b) Vetorizar\n",
    "    input_vec = source_vectorization([cleaned])\n",
    "    print(f\"   2. Vetoriza√ß√£o: shape {input_vec.shape}, primeiros tokens: {input_vec.numpy()[0][:5]}\")\n",
    "    \n",
    "    # c) Criar m√°scaras\n",
    "    enc_mask = create_padding_mask(input_vec)\n",
    "    print(f\"   3. M√°scara encoder: shape {enc_mask.shape}\")\n",
    "    \n",
    "    # d) Passar pelo encoder\n",
    "    encoder_out = transformer.encoder(input_vec, training=False, mask=enc_mask)\n",
    "    print(f\"   4. Encoder output: shape {encoder_out.shape}\")\n",
    "    \n",
    "    # e) Preparar decoder input com token [start]\n",
    "    dec_input = tf.constant([[start_token_index]], dtype=tf.int64)\n",
    "    print(f\"   5. Decoder input inicial: {dec_input.numpy()}, palavra: '{index_to_word[start_token_index]}'\")\n",
    "    \n",
    "    # f) Criar m√°scaras do decoder\n",
    "    look_ahead, dec_pad = create_decoder_masks(dec_input, input_vec)\n",
    "    print(f\"   6. M√°scaras decoder: look_ahead {look_ahead.shape}, padding {dec_pad.shape}\")\n",
    "    \n",
    "    # g) Passar pelo decoder\n",
    "    decoder_out = transformer.decoder(dec_input, encoder_out, training=False,\n",
    "                                      look_ahead_mask=look_ahead, padding_mask=dec_pad)\n",
    "    print(f\"   7. Decoder output: shape {decoder_out.shape}\")\n",
    "    \n",
    "    # h) Camada final\n",
    "    predictions = transformer.final_layer(decoder_out)\n",
    "    print(f\"   8. Predictions: shape {predictions.shape}\")\n",
    "    \n",
    "    # i) Pegar token previsto\n",
    "    predicted_id = tf.argmax(predictions[0, -1, :]).numpy()\n",
    "    predicted_word = index_to_word.get(predicted_id, '???')\n",
    "    print(f\"   9. Token previsto: ID={predicted_id}, palavra='{predicted_word}'\")\n",
    "    \n",
    "    print(\"\\n‚úÖ Pipeline de infer√™ncia completo funciona!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Pipeline de infer√™ncia com erro: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üìä RESUMO: Teste de depend√™ncias conclu√≠do\")\n",
    "print(\"=\" * 70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c745e7a",
   "metadata": {},
   "source": [
    "### Diagn√≥stico do Modelo (Verifica√ß√£o de Colapso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7cbc85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DIAGN√ìSTICO: Vamos verificar se o modelo est√° realmente treinado\n",
    "# ============================================================================\n",
    "\n",
    "print(\"üîç DIAGN√ìSTICO DO MODELO\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# 1. Verificar se o modelo tem pesos carregados\n",
    "print(\"\\n1. Verificando pesos do modelo:\")\n",
    "encoder_weights = transformer.encoder.get_weights()\n",
    "decoder_weights = transformer.decoder.get_weights()\n",
    "print(f\"   ‚úÖ Encoder tem {len(encoder_weights)} tensores de pesos\")\n",
    "print(f\"   ‚úÖ Decoder tem {len(decoder_weights)} tensores de pesos\")\n",
    "\n",
    "# 2. Verificar distribui√ß√£o de probabilidades do modelo\n",
    "print(\"\\n2. Verificando distribui√ß√£o de probabilidades:\")\n",
    "test_input = source_vectorization([\"eu\"])\n",
    "test_decoder = tf.constant([[start_token_index]], dtype=tf.int64)\n",
    "\n",
    "# Processar com o modelo\n",
    "enc_mask = create_padding_mask(test_input)\n",
    "encoder_out = transformer.encoder(test_input, training=False, mask=enc_mask)\n",
    "look_ahead, dec_mask = create_decoder_masks(test_decoder, test_input)\n",
    "decoder_out = transformer.decoder(test_decoder, encoder_out, training=False, \n",
    "                                   look_ahead_mask=look_ahead, padding_mask=dec_mask)\n",
    "predictions = transformer.final_layer(decoder_out)\n",
    "\n",
    "# Verificar as top-5 predi√ß√µes\n",
    "probs = tf.nn.softmax(predictions[0, 0, :])\n",
    "top_5_indices = tf.argsort(probs, direction='DESCENDING')[:5].numpy()\n",
    "top_5_probs = tf.gather(probs, top_5_indices).numpy()\n",
    "\n",
    "print(f\"   Top 5 palavras previstas para 'eu':\")\n",
    "for idx, (token_id, prob) in enumerate(zip(top_5_indices, top_5_probs)):\n",
    "    word = index_to_word.get(token_id, '???')\n",
    "    print(f\"      {idx+1}. '{word}' (ID={token_id}): {prob:.4f}\")\n",
    "\n",
    "# 3. Verificar se o modelo est√° colapsado (sempre prev√™ o mesmo)\n",
    "print(\"\\n3. Testando varia√ß√£o nas predi√ß√µes:\")\n",
    "test_sentences = [\"eu\", \"voc√™\", \"ele\", \"n√≥s\"]\n",
    "predictions_per_word = []\n",
    "\n",
    "for sent in test_sentences:\n",
    "    test_in = source_vectorization([sent])\n",
    "    dec_in = tf.constant([[start_token_index]], dtype=tf.int64)\n",
    "    \n",
    "    enc_mask = create_padding_mask(test_in)\n",
    "    enc_out = transformer.encoder(test_in, training=False, mask=enc_mask)\n",
    "    la_mask, dec_mask = create_decoder_masks(dec_in, test_in)\n",
    "    dec_out = transformer.decoder(dec_in, enc_out, training=False, \n",
    "                                   look_ahead_mask=la_mask, padding_mask=dec_mask)\n",
    "    pred = transformer.final_layer(dec_out)\n",
    "    \n",
    "    predicted_token = tf.argmax(pred[0, 0, :]).numpy()\n",
    "    predicted_word = index_to_word.get(predicted_token, '???')\n",
    "    predictions_per_word.append((sent, predicted_word, predicted_token))\n",
    "    print(f\"   '{sent}' ‚Üí '{predicted_word}' (ID={predicted_token})\")\n",
    "\n",
    "# 4. Diagn√≥stico final\n",
    "print(\"\\n4. Diagn√≥stico:\")\n",
    "unique_predictions = len(set([p[2] for p in predictions_per_word]))\n",
    "if unique_predictions == 1:\n",
    "    print(\"   ‚ö†Ô∏è  PROBLEMA: Modelo colapsado! Sempre prev√™ o mesmo token.\")\n",
    "    print(\"   üîß SOLU√á√ÉO:\")\n",
    "    print(\"      1. Treinar por mais √©pocas (pelo menos 20-30)\")\n",
    "    print(\"      2. Verificar se houve overfitting no 'if' durante treinamento\")\n",
    "    print(\"      3. Considerar reduzir o vocabul√°rio (max_tokens=5000)\")\n",
    "    print(\"      4. Aumentar o modelo (d_model=256, dff=512)\")\n",
    "else:\n",
    "    print(f\"   ‚úÖ Modelo gera {unique_predictions} tokens diferentes\")\n",
    "    print(\"   ‚ÑπÔ∏è  Mas ainda pode precisar de mais treinamento\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06dabd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import tensorflow as tf\n",
    "\n",
    "# Mostra o caminho para o execut√°vel do Python que o notebook est√° usando\n",
    "print(\"Python Executable:\", sys.executable)\n",
    "\n",
    "# Mostra a vers√£o do TensorFlow que foi importada\n",
    "print(\"TensorFlow Version:\", tf.__version__)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
